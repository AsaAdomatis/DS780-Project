{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some libraries\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "sys.path.insert(1, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import darts\n",
    "from darts import metrics\n",
    "\n",
    "from lib.gluformer.model import *\n",
    "from lib.latent_ode.trainer_glunet import *\n",
    "from utils.darts_processing import *\n",
    "from utils.darts_dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glucose plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots of uninterpolated glucose data for each patient\n",
    "formatter, series, scalers = load_data(seed=0, study_file=None, dataset='weinstock', use_covs=True, use_static_covs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data to plot: patients with more than 3 segments\n",
    "plot_data = []\n",
    "for pateint_id, patient_data in formatter.data.groupby('id'):\n",
    "    # count number of unique segments, id_segment\n",
    "    n_segment = patient_data['id_segment'].nunique()\n",
    "    if n_segment > 3:\n",
    "        plot_data.append(patient_data)\n",
    "        if len(plot_data) == 3:\n",
    "            break\n",
    "# for each patient, create a column with number of minutes since the absolute start\n",
    "#    and convert to long format for plotting\n",
    "for i, patient_data in enumerate(plot_data):\n",
    "    patient_data['time'] = patient_data['time'] - patient_data['time'].min()\n",
    "    # convert time to float, number of hours\n",
    "    patient_data['time'] = patient_data['time'].dt.total_seconds() / 3600\n",
    "    # convert to long format for plotting\n",
    "    plot_data[i] = patient_data.melt(id_vars=['time', 'id_segment'], value_vars=['gl'], var_name='y', value_name='x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use whitegrid style\n",
    "sns.set_style('whitegrid')\n",
    "colors = ['#000000', '#003DFD', '#b512b8', '#11a9ba', '#0d780f', '#f77f07', '#ba0f0f']\n",
    "# plot using seaborn\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "for i, patient_data in enumerate(plot_data):\n",
    "    sns.lineplot(x='time', y='x', hue='id_segment', \n",
    "                 data=patient_data, ax=axes[i],\n",
    "                 palette=colors)\n",
    "    axes[i].set_ylabel('Glucose (mg/dL)')\n",
    "    # remove legend\n",
    "    axes[i].get_legend().remove()\n",
    "    # only add xlabel to last plot\n",
    "    axes[i].set_xlabel('')\n",
    "    if i == 2:\n",
    "        axes[i].set_xlabel('Time (hours)')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and interpolation example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Define the colors for the rectangles\n",
    "colors = ['#FEE1C7', '#C8FEC7', '#DAF9FE', '#FEF4FF']\n",
    "labels = ['Train', 'Validation', 'ID Test', 'OOD Test']\n",
    "\n",
    "# plot glucose data for each id\n",
    "fig, axs = plt.subplots(3, 1, figsize=(20, 15))\n",
    "for i, id in enumerate(ids[:3]):\n",
    "    for j in range(len(id_series[id][:3])):\n",
    "        id_series[id][j].plot(ax=axs[i], color='black')  # Set the line color to black\n",
    "\n",
    "    if i < len(ids[:3]) - 1:  # Only create rectangles for the first two plots\n",
    "        y_min, y_max = axs[i].get_ylim()\n",
    "        x_min, x_max = axs[i].get_xlim()\n",
    "        total_width = x_max - x_min\n",
    "        first_width = total_width * 2/3\n",
    "        second_width = (total_width - first_width) / 2\n",
    "\n",
    "        for k, color in enumerate(colors):\n",
    "            if k == 0:\n",
    "                width = first_width\n",
    "            else:\n",
    "                width = second_width\n",
    "            rect = Rectangle((x_min + sum(width for width in [first_width, second_width][:k]), y_min),\n",
    "                             width, y_max - y_min, facecolor=color, alpha=0.5)\n",
    "            axs[i].add_patch(rect)\n",
    "    else:  # Set the background color of the last plot to #D6EDFE\n",
    "        axs[i].set_facecolor('#FEF4FF')\n",
    "\n",
    "    axs[i].set_ylabel('Glucose (mg/dL)')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].set_title(f'Glucose data for patient {i+1}')\n",
    "    axs[i].tick_params(axis='x', rotation=0)\n",
    "    if axs[i].get_legend() is not None:\n",
    "        axs[i].get_legend().remove()\n",
    "    # increase font size\n",
    "    for item in ([axs[i].title, axs[i].xaxis.label, axs[i].yaxis.label] +\n",
    "                axs[i].get_xticklabels() + axs[i].get_yticklabels()):\n",
    "        item.set_fontsize(22)\n",
    "\n",
    "# Create a custom legend for the colors\n",
    "legend_elements = [Rectangle((0, 0), 1, 1, facecolor=color, edgecolor=color, label=label)\n",
    "                   for color, label in zip(colors, labels)]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=len(colors), fontsize=22, frameon=True)\n",
    "\n",
    "# save as pdf with transparent background\n",
    "plt.savefig('presentation/plots/glucose_data_split_plot.pdf', bbox_inches='tight', transparent=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example plots of other Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load electrcity data \n",
    "from darts.datasets import TemperatureDataset\n",
    "from darts.datasets import AirPassengersDataset\n",
    "from darts.datasets import HeartRateDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(30, 15))\n",
    "dataset_names = ['Temperature', 'AirPassengers', 'HeartRate']\n",
    "yaxis = ['Temperature (C)', 'AirPassengers', 'HeartRate (bpm)']\n",
    "for i, dataset in enumerate([TemperatureDataset(), AirPassengersDataset(), HeartRateDataset()]):\n",
    "    dataset.load().plot(ax=axs[i])\n",
    "    axs[i].set_ylabel(f'{yaxis[i]}')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(axis='x', rotation=0)\n",
    "    axs[i].set_title(f'{dataset_names[i]}')\n",
    "    if axs[i].get_legend() is not None:\n",
    "        axs[i].get_legend().remove()\n",
    "    # increase font size\n",
    "    for item in ([axs[i].title, axs[i].xaxis.label, axs[i].yaxis.label] +\n",
    "                axs[i].get_xticklabels() + axs[i].get_yticklabels()):\n",
    "        item.set_fontsize(22)\n",
    "# save as pdf\n",
    "plt.savefig('presentation/plots/ts_data.pdf', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save forecasts of all models: no covariates, ID test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "model_params = {'transformer': {'darts': models.TransformerModel, 'darts_data': SamplingDatasetInferencePast, 'use_covs': False, 'use_static_covs': False, 'cov_type': 'past'},\n",
    "                'nhits': {'darts': models.NHiTSModel, 'darts_data': SamplingDatasetInferencePast, 'use_covs': False, 'use_static_covs': False, 'cov_type': 'past'},\n",
    "                'tft': {'darts': models.TFTModel, 'darts_data': SamplingDatasetInferenceMixed, 'use_covs': False, 'use_static_covs': True, 'cov_type': 'mixed'},\n",
    "                'xgboost': {'darts': models.XGBModel, 'use_covs': False, 'use_static_covs': False, 'cov_type': 'past'},\n",
    "                'linreg': {'darts': models.LinearRegressionModel, 'use_covs': False, 'use_static_covs': False, 'cov_type': 'past'}}\n",
    "# data sets\n",
    "datasets = ['weinstock', 'dubosson', 'colas', 'iglu', 'hall']\n",
    "save_trues = {}\n",
    "save_forecasts = {}\n",
    "# iterate through models and datasets\n",
    "for model_name in model_params.keys():\n",
    "    for dataset in datasets:\n",
    "        print(f'Testing {model_name} for {dataset}')\n",
    "        formatter, series, scalers = load_data(seed=0, study_file=None, dataset=dataset, \n",
    "                                               use_covs=model_params[model_name]['use_covs'], \n",
    "                                             use_static_covs=model_params[model_name]['use_static_covs'],\n",
    "                                             cov_type=model_params[model_name]['cov_type'])\n",
    "        # load model or refit model\n",
    "        if model_name in ['tft', 'transformer', 'nhits']:\n",
    "            # load model: transformer\n",
    "            model = model_params[model_name]['darts'](input_chunk_length=formatter.params[model_name]['in_len'],\n",
    "                                              output_chunk_length=formatter.params['length_pred'])\n",
    "            model = model.load_from_checkpoint(f'tensorboard_{model_name}_{dataset}', work_dir = './output', best=True)\n",
    "            # define dataset for inference\n",
    "            test_dataset = model_params[model_name]['darts_data'](target_series=series['test']['target'],\n",
    "                                                              n=formatter.params['length_pred'],\n",
    "                                                                input_chunk_length=formatter.params[model_name]['in_len'],\n",
    "                                                                  output_chunk_length=formatter.params['length_pred'],\n",
    "                                                                  use_static_covariates=model_params[model_name]['use_static_covs'],\n",
    "                                                                  max_samples_per_ts = None)\n",
    "            # get predictions\n",
    "            forecasts = model.predict_from_dataset(n=formatter.params['length_pred'], \n",
    "                                                   input_series_dataset=test_dataset,\n",
    "                                                   verbose=True,\n",
    "                                                   num_samples=20 if model_name == 'tft' else 1)\n",
    "            forecasts = scalers['target'].inverse_transform(forecasts)\n",
    "            save_forecasts[f'{model_name}_{dataset}'] = forecasts\n",
    "            # get true values\n",
    "            save_trues[f'{model_name}_{dataset}'] = [test_dataset.evalsample(i) for i in range(len(test_dataset))]\n",
    "            save_trues[f'{model_name}_{dataset}'] = scalers['target'].inverse_transform(save_trues[f'{model_name}_{dataset}'])\n",
    "        elif model_name == 'xgboost':\n",
    "            # load model: xgboost\n",
    "            model = model_params[model_name]['darts'](lags=formatter.params[model_name]['in_len'], \n",
    "                                                      learning_rate=formatter.params[model_name]['lr'],\n",
    "                                                      subsample=formatter.params[model_name]['subsample'],\n",
    "                                                      min_child_weight=formatter.params[model_name]['min_child_weight'],\n",
    "                                                      colsample_bytree=formatter.params[model_name]['colsample_bytree'],\n",
    "                                                      max_depth=formatter.params[model_name]['max_depth'],\n",
    "                                                      gamma=formatter.params[model_name]['gamma'],\n",
    "                                                      reg_alpha=formatter.params[model_name]['alpha'],\n",
    "                                                      reg_lambda=formatter.params[model_name]['lambda_'],\n",
    "                                                      n_estimators=formatter.params[model_name]['n_estimators'],\n",
    "                                                      random_state=0)\n",
    "            # fit model\n",
    "            model.fit(series['train']['target'])\n",
    "            # get predictions\n",
    "            forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                                   forecast_horizon=formatter.params['length_pred'],\n",
    "                                                   stride=1,\n",
    "                                                   retrain=False,\n",
    "                                                   verbose=True,\n",
    "                                                   last_points_only=False,\n",
    "                                                   start=formatter.params[\"max_length_input\"])\n",
    "            forecasts = [scalers['target'].inverse_transform(forecast) for forecast in forecasts]\n",
    "            save_forecasts[f'{model_name}_{dataset}'] = forecasts\n",
    "            # get true values\n",
    "            save_trues[f'{model_name}_{dataset}'] = scalers['target'].inverse_transform(series['test']['target'])\n",
    "        elif model_name == 'linreg':\n",
    "            # load model: linear regression\n",
    "            model = models.LinearRegressionModel(lags = formatter.params[model_name]['in_len'],\n",
    "                                                 output_chunk_length = formatter.params['length_pred'])\n",
    "            model.fit(series['train']['target'])\n",
    "            # get predictions\n",
    "            forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                                forecast_horizon=formatter.params['length_pred'], \n",
    "                                                stride=1,\n",
    "                                                retrain=False,\n",
    "                                                verbose=False,\n",
    "                                                last_points_only=False,\n",
    "                                                start=formatter.params[\"max_length_input\"])\n",
    "            forecasts = [scalers['target'].inverse_transform(forecast) for forecast in forecasts]\n",
    "            save_forecasts[f'{model_name}_{dataset}'] = forecasts\n",
    "            # get true values\n",
    "            save_trues[f'{model_name}_{dataset}'] = scalers['target'].inverse_transform(series['test']['target'])\n",
    "\n",
    "            \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['weinstock', 'dubosson', 'colas', 'iglu', 'hall']\n",
    "device = 'cuda'\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f'Testing {dataset}')\n",
    "    formatter, series, scalers = load_data(seed=0, study_file=None, dataset=dataset, use_covs=True, use_static_covs=True)\n",
    "    # define dataset for inference: gluformer\n",
    "    dataset_test_glufo = SamplingDatasetInferenceDual(target_series=series['test']['target'],\n",
    "                                                      covariates=series['test']['future'],\n",
    "                                                      input_chunk_length=formatter.params['gluformer']['in_len'],\n",
    "                                                      output_chunk_length=formatter.params['length_pred'],\n",
    "                                                      use_static_covariates=True,\n",
    "                                                      array_output_only=True)\n",
    "    # define dataset for inference: latent ode\n",
    "    dataset_test_latod = SamplingDatasetInferenceDual(target_series=series['test']['target'],\n",
    "                                                      covariates=series['test']['future'],\n",
    "                                                      input_chunk_length=formatter.params['latentode']['in_len'],\n",
    "                                                      output_chunk_length=formatter.params['length_pred'],\n",
    "                                                      use_static_covariates=True,\n",
    "                                                      array_output_only=True)\n",
    "    # load model: gluformer\n",
    "    num_dynamic_features = series['train']['future'][-1].n_components\n",
    "    num_static_features = series['train']['static'][-1].n_components\n",
    "    glufo = Gluformer(d_model = formatter.params['gluformer']['d_model'],\n",
    "                      n_heads = formatter.params['gluformer']['n_heads'],\n",
    "                      d_fcn = formatter.params['gluformer']['d_fcn'],\n",
    "                      r_drop = 0.2, \n",
    "                      activ = 'relu', \n",
    "                      num_enc_layers = formatter.params['gluformer']['num_enc_layers'], \n",
    "                      num_dec_layers = formatter.params['gluformer']['num_dec_layers'],\n",
    "                      distil = True, \n",
    "                      len_seq = formatter.params['gluformer']['in_len'],\n",
    "                      label_len = formatter.params['gluformer']['in_len'] // 3,\n",
    "                      len_pred = formatter.params['length_pred'],\n",
    "                      num_dynamic_features = num_dynamic_features,\n",
    "                      num_static_features = num_static_features,)\n",
    "    glufo.to(device)\n",
    "    glufo.load_state_dict(torch.load(f'./output/tensorboard_gluformer_{dataset}/model.pt', map_location=torch.device(device)))\n",
    "    # load model: latent ode\n",
    "    latod = LatentODEWrapper(device = device,\n",
    "                             latents = formatter.params['latentode']['latents'],\n",
    "                             rec_dims = formatter.params['latentode']['rec_dims'],\n",
    "                             rec_layers = formatter.params['latentode']['rec_layers'],\n",
    "                             gen_layers = formatter.params['latentode']['gen_layers'],\n",
    "                             units = formatter.params['latentode']['units'],\n",
    "                             gru_units = formatter.params['latentode']['gru_units'],)\n",
    "    latod.load(f'./output/tensorboard_latentode_{dataset}/model.ckpt', device)\n",
    "    # get predictions: gluformer\n",
    "    print('Gluformer')\n",
    "    forecasts, _ = glufo.predict(dataset_test_glufo,\n",
    "                                 batch_size=8,\n",
    "                                 num_samples=10,\n",
    "                                 device=device,\n",
    "                                 use_tqdm=True)\n",
    "    forecasts = (forecasts - scalers['target'].min_) / scalers['target'].scale_\n",
    "    trues = [dataset_test_glufo.evalsample(i) for i in range(len(dataset_test_glufo))]\n",
    "    trues = scalers['target'].inverse_transform(trues)\n",
    "    save_forecasts[f'gluformer_{dataset}'] = forecasts\n",
    "    save_trues[f'gluformer_{dataset}'] = trues\n",
    "    # get predictions: latent ode\n",
    "    print('Latent ODE')\n",
    "    forecasts = latod.predict(dataset_test_latod,\n",
    "                              batch_size=32,\n",
    "                              num_samples=20,\n",
    "                              device=device,\n",
    "                              use_tqdm=True,)\n",
    "    forecasts = (forecasts - scalers['target'].min_) / scalers['target'].scale_\n",
    "    trues = [dataset_test_latod.evalsample(i) for i in range(len(dataset_test_latod))]\n",
    "    trues = scalers['target'].inverse_transform(trues)\n",
    "    save_forecasts[f'latentode_{dataset}'] = forecasts\n",
    "    save_trues[f'latentode_{dataset}'] = trues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save forecasts\n",
    "with gzip.open('./presentation/data/compressed_forecasts.pkl', 'wb') as file:\n",
    "    pickle.dump(save_forecasts, file)\n",
    "# save true values\n",
    "with gzip.open('./presentation/data/compressed_trues.pkl', 'wb') as file:\n",
    "    pickle.dump(save_trues, file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute day and night-time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load forecasts\n",
    "with gzip.open('./presentation/data/compressed_forecasts.pkl', 'rb') as file:\n",
    "    save_forecasts = pickle.load(file)\n",
    "# load true values\n",
    "with gzip.open('./presentation/data/compressed_trues.pkl', 'rb') as file:\n",
    "    save_trues = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_errors = {}\n",
    "night_errors = {}\n",
    "datasets = ['weinstock', 'dubosson', 'colas', 'iglu', 'hall']\n",
    "models = ['gluformer', 'latentode', 'tft', 'nhits', 'linreg', 'xgboost', 'transformer']\n",
    "\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        # initialize errors\n",
    "        day_errors[f'{model}_{dataset}'] = []\n",
    "        night_errors[f'{model}_{dataset}'] = []\n",
    "        # get forecasts\n",
    "        forecasts = save_forecasts[f'{model}_{dataset}']\n",
    "        # get true values\n",
    "        trues = save_trues[f'{model}_{dataset}']\n",
    "        # compute errors\n",
    "        if model in ['xgboost', 'linreg']:\n",
    "            for i, fs in enumerate(forecasts):\n",
    "                t = trues[i]\n",
    "                for f in fs:\n",
    "                    hour = f.start_time().hour\n",
    "                    error = metrics.mae(f, t)\n",
    "                    if hour >= 9 and hour < 21:\n",
    "                        day_errors[f'{model}_{dataset}'].append(error)\n",
    "                    else:\n",
    "                        night_errors[f'{model}_{dataset}'].append(error)\n",
    "        elif model in ['tft', 'nhits', 'transformer']:\n",
    "            for i, f in enumerate(forecasts):\n",
    "                # extract true value and hour\n",
    "                t = trues[i]\n",
    "                hour = t.start_time().hour\n",
    "                # compute error\n",
    "                error = metrics.mae(f, t)\n",
    "                if hour >= 9 and hour < 21:\n",
    "                    day_errors[f'{model}_{dataset}'].append(error)\n",
    "                else:\n",
    "                    night_errors[f'{model}_{dataset}'].append(error)\n",
    "        else:\n",
    "            for i, t in enumerate(trues):\n",
    "                # extract hour and true as array\n",
    "                hour = t.start_time().hour\n",
    "                t = t.values().squeeze()\n",
    "                # get the forecast\n",
    "                if model == 'gluformer':\n",
    "                    f = np.mean(forecasts[i, :, :], axis=1)\n",
    "                elif model == 'latentode':\n",
    "                    f = np.mean(forecasts[:, i, :, 0], axis=0)\n",
    "                # compute error\n",
    "                error = np.mean(np.abs(f - t))\n",
    "                if hour >= 9 and hour < 21:\n",
    "                    day_errors[f'{model}_{dataset}'].append(error)  \n",
    "                else:\n",
    "                    night_errors[f'{model}_{dataset}'].append(error)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save day and night errors\n",
    "with gzip.open('./presentation/data/compressed_day_errors.pkl', 'wb') as file:\n",
    "    pickle.dump(day_errors, file)\n",
    "with gzip.open('./presentation/data/compressed_night_errors.pkl', 'wb') as file:\n",
    "    pickle.dump(night_errors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "day_errors_list = day_errors[f'transformer_weinstock']\n",
    "night_errors_list = night_errors[f'transformer_weinstock']\n",
    "# plot normalized histogram with 50 bins\n",
    "sns.histplot(day_errors_list, stat='density', alpha=0.5, \n",
    "        color='blue', label='Day Errors', ax=ax, bins=35)\n",
    "sns.histplot(night_errors_list, stat='density', alpha=0.5, \n",
    "        color='darkgreen', label='Night Errors', ax=ax, bins=35)\n",
    "# Plot density estimates of the two lists of values\n",
    "sns.kdeplot(day_errors_list, color='blue', linestyle='-', ax=ax)\n",
    "sns.kdeplot(night_errors_list, color='darkgreen', linestyle='-', ax=ax)\n",
    "# Add legend\n",
    "ax.legend()\n",
    "# set y label for first column\n",
    "if j == 0:\n",
    "        ax.set_ylabel('Density')\n",
    "else:\n",
    "        ax.set_ylabel('')\n",
    "# set x label for last row\n",
    "if i == len(models) - 1:\n",
    "        ax.set_xlabel('MAE')\n",
    "else:\n",
    "        ax.set_xlabel('')\n",
    "# add legend to first plot\n",
    "ax.legend()\n",
    "# increase font size of x and y labels, title and legend\n",
    "for item in ([ax.xaxis.label, ax.yaxis.label] +\n",
    "                ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(16)\n",
    "if ax.get_legend() is not None:\n",
    "        for item in ax.get_legend().get_texts():\n",
    "                item.set_fontsize(12)\n",
    "# save as pdf\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./presentation/plots/night_vs_day_error_transformer_weinstock.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot density and normalized histogram of day / night errors for all models and datasets\n",
    "fig, axs = plt.subplots(len(models), len(datasets), figsize=(30, 40))\n",
    "for i, model in enumerate(models):\n",
    "        for j, dataset in enumerate(datasets):\n",
    "                ax = axs[i, j]\n",
    "                day_errors_list = day_errors[f'{model}_{dataset}']\n",
    "                night_errors_list = night_errors[f'{model}_{dataset}']\n",
    "                # plot normalized histogram with 50 bins\n",
    "                sns.histplot(day_errors_list, stat='density', alpha=0.5, \n",
    "                        color='blue', label='Day Errors', ax=ax, bins=35)\n",
    "                sns.histplot(night_errors_list, stat='density', alpha=0.5, \n",
    "                        color='darkgreen', label='Night Errors', ax=ax, bins=35)\n",
    "                # Plot density estimates of the two lists of values\n",
    "                sns.kdeplot(day_errors_list, color='blue', linestyle='-', ax=ax)\n",
    "                sns.kdeplot(night_errors_list, color='darkgreen', linestyle='-', ax=ax)\n",
    "                # Add legend\n",
    "                ax.legend()\n",
    "                # set y label for first column\n",
    "                if j == 0:\n",
    "                        ax.set_ylabel('Density')\n",
    "                else:\n",
    "                        ax.set_ylabel('')\n",
    "                # set x label for last row\n",
    "                if i == len(models) - 1:\n",
    "                        ax.set_xlabel('MAE')\n",
    "                else:\n",
    "                        ax.set_xlabel('')\n",
    "                # set title\n",
    "                ax.set_title(f'{model.upper()} - {dataset.upper()}')\n",
    "                # add legend to first plot\n",
    "                if i == 0 and j == 0:\n",
    "                        ax.legend()\n",
    "                else: \n",
    "                        ax.get_legend().remove()\n",
    "                # increase font size of x and y labels, title and legend\n",
    "                for item in ([ax.xaxis.label, ax.yaxis.label] +\n",
    "                                ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "                        item.set_fontsize(16)\n",
    "                if ax.get_legend() is not None:\n",
    "                        for item in ax.get_legend().get_texts():\n",
    "                                item.set_fontsize(12)\n",
    "# save as pdf\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./presentation/plots/night_vs_day_error.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tail simulation to check Gaussianity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/m/mrsergazinov/.conda/envs/ccnf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import data_formatters.utils as utils\n",
    "from dataset import TSDataset\n",
    "from conf import Conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code walk-through\n",
    "\n",
    "The major parts of the code that need to be defined for each data set are:\n",
    "1. config file in `.yaml` format,\n",
    "2. data formatter script.\n",
    "\n",
    "For now, you can study the `electricity.yaml` example for a look of what a config file should feel like. You can skip the hyperparam defintions and the model parameters. The main focus would be on defining the dataset parameters. \n",
    "\n",
    "We do not intereact with `.yaml` in a direct way but instead though `Conf` class, which handles the following:\n",
    "1. defines some defaults if not specified in `.yaml`,\n",
    "2. sets save paths,\n",
    "3. allows for nice colored printing.\n",
    "\n",
    "Technically, we could doo all of this in the `.yaml` file directly. However, then every time we re-run the experiment, we would have to manually modify the `.yaml` file to reset save paths and redefine some variables, which would be inconvenient.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the config file, setting the experiment name, and the seed for random pre-processing parts (like splitting)\n",
    "cnf = Conf(conf_file_path='./conf/iglu_urjeet.yaml', seed=15, exp_name=\"test\", log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default configuration parameters: \n",
      "\u001b[34mLR\u001b[0m\u001b[31m: \u001b[0m\u001b[35m0.001\u001b[0m\n",
      "\u001b[34mEPOCHS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m20\u001b[0m\n",
      "\u001b[34mN_WORKERS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m0\u001b[0m\n",
      "\u001b[34mBATCH_SIZE\u001b[0m\u001b[31m: \u001b[0m\u001b[35m64\u001b[0m\n",
      "\u001b[34mQUANTILES\u001b[0m\u001b[31m: \u001b[0m\u001b[35m[0.1, 0.5, 0.9]\u001b[0m\n",
      "\u001b[34mDS_NAME\u001b[0m\u001b[31m: \u001b[0m\u001b[33miglu_urjeet\u001b[0m\n",
      "\u001b[34mALL_PARAMS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m{'ds_name': 'iglu_urjeet', 'data_csv_path': './raw_data/iglu_example_data_5_subject.csv', 'index_col': -1, 'total_time_steps': 192, 'num_encoder_steps': 168, 'max_samples': 0, 'batch_size': 64, 'device': 'cuda', 'lr': 0.001, 'num_epochs': 20, 'n_workers': 0, 'model': 'transformer', 'loader': 'base', 'quantiles': [0.1, 0.5, 0.9], 'batch_first': True, 'early_stopping_patience': 5, 'hidden_layer_size': 160, 'stack_size': 1, 'dropout_rate': 0.1, 'max_gradient_norm': 0.01, 'num_heads': 4, 'd_model': 64, 'q': 16, 'v': 16, 'h': 4, 'N': 2, 'attention_size': 0, 'dropout': 0.1, 'pe': 'original', 'chunk_mode': 'None', 'd_input': 5, 'd_output': 3}\u001b[0m\n",
      "\u001b[34mEXP_LOG_PATH\u001b[0m\u001b[31m: \u001b[0m\u001b[33m./log/transformer/test/09/16/2022.13:40:09\u001b[0m\n",
      "\u001b[34mDEVICE\u001b[0m\u001b[31m: \u001b[0m\u001b[33mcuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# lets print out the config file\n",
    "print(f'\\nDefault configuration parameters: \\n{cnf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the data formatter. This is the part that should handle:\n",
    "1. loading the data and setting types,\n",
    "2. splitting the data into train / val / test sets,\n",
    "3. setting scalers and encoders for numerical / categorical variables resp.\n",
    "\n",
    "We are going to leave parts 2-3 for the future exploration. Now, let's focus on loading and settting the types for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data formatter script for electricity is in electricity.py \n",
    "# we are going to use a helper function to get a pointer to the data formatter class for our dataset\n",
    "data_formatter = utils.make_data_formatter(cnf.ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's work with the `TSDataset` class. This is the main part of the code as it aligns all of our previous steps. In the end, it is the `TSDataset` that is going to call the splitters, scalers, and encoders. **Importatnly** the model is only going to interact with the data through this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting valid sampling locations.\n",
      "# available segments=12911\n",
      "Extracting all available segments.\n",
      "1000 of 12911 samples done...\n",
      "2000 of 12911 samples done...\n",
      "3000 of 12911 samples done...\n",
      "4000 of 12911 samples done...\n",
      "5000 of 12911 samples done...\n",
      "6000 of 12911 samples done...\n",
      "7000 of 12911 samples done...\n",
      "8000 of 12911 samples done...\n",
      "9000 of 12911 samples done...\n",
      "10000 of 12911 samples done...\n",
      "11000 of 12911 samples done...\n",
      "12000 of 12911 samples done...\n"
     ]
    }
   ],
   "source": [
    "# we are going to pass our data formatter and the config file to the TSDataset class\n",
    "dataset = TSDataset(cnf, data_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #0: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #1: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #2: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #3: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #4: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #5: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #6: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #7: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #8: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #9: x.shape=(192, 1), y.shape=(192, 1)\n"
     ]
    }
   ],
   "source": [
    "# now let's see how we can sample minibatches from our dataset that we can then pass to the model to train on\n",
    "for i in range(10):\n",
    "    # 192 x ['power_usage', 'hour', 'day_of_week', 'hours_from_start', 'categorical_id']\n",
    "    x = dataset[i]['inputs']\n",
    "    # 24 x ['power_usage']\n",
    "    y = dataset[i]['outputs']\n",
    "    print(f'Example #{i}: x.shape={x.shape}, y.shape={y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ccnf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d3b5751fa94629b95e14ad74e847871e830701bb722861c9db8f38e21e4bb6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

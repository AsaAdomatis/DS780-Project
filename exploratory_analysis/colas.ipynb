{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFh18yX0FrjN"
      },
      "source": [
        "# Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eawH97OBFrjS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "sys.path.insert(1, '..')\n",
        "os.chdir('..')\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import sklearn\n",
        "import optuna\n",
        "\n",
        "from darts import models\n",
        "from darts import metrics\n",
        "from darts import TimeSeries\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "\n",
        "from statsforecast.models import AutoARIMA\n",
        "\n",
        "from data_formatter.base import *\n",
        "from bin.utils import *"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Processing raw data and adding covariates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filenames = []\n",
        "for root, dir, files in os.walk('raw_data/Colas2019'):\n",
        "  for file in files:\n",
        "    if '.csv' in file:\n",
        "      filenames.append(os.path.join(root, file))\n",
        "      \n",
        "# next we loop through each file\n",
        "nfiles = len(files)\n",
        "\n",
        "count = 0\n",
        "for file in filenames:\n",
        "  # read in data and extract id from filename\n",
        "  curr = pd.read_csv(file)\n",
        "  curr['id'] = int(file.split()[1].split(\".\")[0])\n",
        "  # select desired columns, rename, and drop nas\n",
        "  curr = curr[['id', 'hora', 'glucemia']]\n",
        "  curr.rename(columns = {'hora': 'time', 'glucemia': 'gl'}, inplace=True)\n",
        "  curr.dropna(inplace=True)\n",
        "\n",
        "  # calculate time (only given in hms) as follows:\n",
        "  # (1) get the time per day in seconds, (2) get the time differences, and correct for the day crossove (< 0)\n",
        "  # (3) take the cumulative sum and add the cumulative number of seconds from start to the base date\n",
        "  # thus the hms are real, while the year, month, day are fake\n",
        "  time_secs = []\n",
        "  for i in curr['time']:\n",
        "      time_secs.append(int(i.split(\":\")[0])*60*60 + int(i.split(\":\")[1])*60 + int(i.split(\":\")[2])*1)\n",
        "  time_diff = np.diff(np.array(time_secs)).tolist()\n",
        "  time_diff_adj = [x if x > 0 else 24*60*60 + x for x in time_diff]\n",
        "  time_diff_adj.insert(0, 0)\n",
        "  cumin = np.cumsum(time_diff_adj)\n",
        "  datetime = pd.to_datetime('2012-01-01') + pd.to_timedelta(cumin, unit='sec')\n",
        "  curr['time'] = datetime\n",
        "  curr['id'] = curr['id'].astype('int')\n",
        "  curr.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  if count == 0:\n",
        "    df = curr\n",
        "    count += 1\n",
        "  else:\n",
        "    df = pd.concat([df, curr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join with covariates\n",
        "covariates = pd.read_csv('raw_data/Colas2019/clinical_data.txt', sep = \" \")\n",
        "covariates['id'] = covariates.index\n",
        "\n",
        "combined = pd.merge(\n",
        "    df, covariates, how = \"left\"\n",
        ")\n",
        "\n",
        "# define NA fill values for covariates\n",
        "values = {\n",
        "    'gender': 2, # if gender is NA, create own category\n",
        "    'age': combined['age'].mean(),\n",
        "    'BMI': combined['BMI'].mean(),\n",
        "    'glycaemia': combined['glycaemia'].mean(),\n",
        "    'HbA1c': combined['HbA1c'].mean(),\n",
        "    'follow.up': combined['follow.up'].mean(),\n",
        "    'T2DM': False\n",
        "}\n",
        "combined = combined.fillna(value = values)\n",
        "\n",
        "# write to csv\n",
        "combined.to_csv('raw_data/colas.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeAHZmAmFrjV"
      },
      "source": [
        "# Check statistics of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkOzK6gcFrjW",
        "outputId": "769510ff-79ba-4020-8d9c-dc78a7cdb7ff"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load yaml config file\n",
        "with open('./config/colas.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# set interpolation params for no interpolation\n",
        "new_config = config.copy()\n",
        "new_config['interpolation_params']['gap_threshold'] = 5\n",
        "new_config['interpolation_params']['min_drop_length'] = 0\n",
        "# set split params for no splitting\n",
        "new_config['split_params']['test_percent_subjects'] = 0\n",
        "new_config['split_params']['length_segment'] = 0\n",
        "# set scaling params for no scaling\n",
        "new_config['scaling_params']['scaler'] = 'None'\n",
        "\n",
        "formatter = DataFormatter(new_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCBgEjuAFrjX",
        "outputId": "1d40e5fa-1fd5-45ea-ae93-41d14226a0c5"
      },
      "outputs": [],
      "source": [
        "# print min, max, median, mean, std of segment lengths\n",
        "segment_lens = []\n",
        "for group, data in formatter.train_data.groupby('id_segment'):\n",
        "    segment_lens.append(len(data))\n",
        "print('Train segment lengths:')\n",
        "print('\\tMin: ', min(segment_lens))\n",
        "print('\\tMax: ', max(segment_lens))\n",
        "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
        "print('\\tMedian: ', np.median(segment_lens))\n",
        "print('\\tMean: ', np.mean(segment_lens))\n",
        "print('\\tStd: ', np.std(segment_lens))\n",
        "\n",
        "# plot first 9 segments\n",
        "num_segments = 9\n",
        "plot_data = formatter.train_data\n",
        "\n",
        "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
        "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
        "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))\n",
        "    if i >= num_segments - 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "iU2AUHTfFrjZ",
        "outputId": "ac25dfa6-4eee-4fc8-9c0c-11efa1b4fa14"
      },
      "outputs": [],
      "source": [
        "# plot acf of random samples from first 9 segments segments\n",
        "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
        "lags = 300; k = 0\n",
        "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
        "    data = data['gl']\n",
        "    if len(data) < lags:\n",
        "        print('Segment {} is too short'.format(group))\n",
        "        continue\n",
        "    else:\n",
        "        # select 10 random samples from index of data\n",
        "        sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
        "        # plot acf / pacf of each sample\n",
        "        for j in sample:\n",
        "            acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
        "            pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
        "            ax[0, k].plot(acf)\n",
        "            ax[1, k].plot(pacf)\n",
        "        k += 1\n",
        "        if k >= num_segments:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QD18En1Frjb"
      },
      "source": [
        "# Change the config according to the observations above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaI4mTzaFrjc",
        "outputId": "56c54fbf-bd7f-47d2-c0af-7e5fe8403817"
      },
      "outputs": [],
      "source": [
        "# set interpolation params for interpolation\n",
        "new_config['interpolation_params']['gap_threshold'] = 45 # minutes - use as in config file \n",
        "new_config['interpolation_params']['min_drop_length'] = 192\n",
        "\n",
        "formatter = DataFormatter(new_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print min, max, median, mean, std of segment lengths\n",
        "segment_lens = []\n",
        "for group, data in formatter.train_data.groupby('id_segment'):\n",
        "    segment_lens.append(len(data))\n",
        "print('Train segment lengths:')\n",
        "print('\\tMin: ', min(segment_lens))\n",
        "print('\\tMax: ', max(segment_lens))\n",
        "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
        "print('\\tMedian: ', np.median(segment_lens))\n",
        "print('\\t3rd Quartile: ', np.quantile(segment_lens, 0.75))\n",
        "print('\\tMean: ', np.mean(segment_lens))\n",
        "print('\\tStd: ', np.std(segment_lens))\n",
        "\n",
        "# plot first 9 segments\n",
        "num_segments = 9\n",
        "plot_data = formatter.train_data\n",
        "\n",
        "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
        "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
        "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))\n",
        "    if i >= num_segments - 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot acf of random samples from first 9 segments segments\n",
        "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
        "lags = 300; k = 0\n",
        "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
        "    data = data['gl']\n",
        "    if len(data) < lags:\n",
        "        print('Segment {} is too short'.format(group))\n",
        "        continue\n",
        "    else:\n",
        "        # select 10 random samples from index of data\n",
        "        sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
        "        # plot acf / pacf of each sample\n",
        "        for j in sample:\n",
        "            acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
        "            pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
        "            ax[0, k].plot(acf)\n",
        "            ax[1, k].plot(pacf)\n",
        "        k += 1\n",
        "        if k >= num_segments:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./config/colas.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "    \n",
        "# set interpolation params for no interpolation\n",
        "config['interpolation_params']['gap_threshold'] = 45\n",
        "config['interpolation_params']['min_drop_length'] = 192\n",
        "# set split params for no splitting\n",
        "config['split_params']['test_percent_subjects'] = 0.1\n",
        "config['split_params']['length_segment'] = 192\n",
        "# set scaling params for no scaling\n",
        "config['scaling_params']['scaler'] = 'None'\n",
        "\n",
        "formatter = DataFormatter(config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z3lSc-DFrjc"
      },
      "source": [
        "# Models (no covariates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wJKLzSMFrjd"
      },
      "source": [
        "## Convert data and (optional) scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "aRkqpyY1Frje",
        "outputId": "82e3456d-f4e3-4acb-ae3b-98de34f4322c"
      },
      "outputs": [],
      "source": [
        "def load_data(seed = 0, study_file = None):\n",
        "    # load data\n",
        "    with open('./config/colas.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    config['split_params']['random_state'] = seed\n",
        "    formatter = DataFormatter(config, study_file = study_file)\n",
        "\n",
        "    # convert to series\n",
        "    time_col = formatter.get_column('time')\n",
        "    group_col = formatter.get_column('sid')\n",
        "    target_col = formatter.get_column('target')\n",
        "    static_cols = formatter.get_column('static_covs')\n",
        "    static_cols = static_cols + [formatter.get_column('id')] if static_cols is not None else [formatter.get_column('id')]\n",
        "    dynamic_cols = formatter.get_column('dynamic_covs')\n",
        "    future_cols = formatter.get_column('future_covs')\n",
        "\n",
        "    # build series\n",
        "    series, scalers = make_series({'train': formatter.train_data,\n",
        "                                    'val': formatter.val_data,\n",
        "                                    'test': formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)],\n",
        "                                    'test_ood': formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]},\n",
        "                                    time_col,\n",
        "                                    group_col,\n",
        "                                    {'target': target_col,\n",
        "                                    'static': static_cols,\n",
        "                                    'dynamic': dynamic_cols,\n",
        "                                    'future': future_cols})\n",
        "    \n",
        "    return formatter, series, scalers\n",
        "\n",
        "formatter, series, scalers = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for f in series['train']['target']:\n",
        "    f.plot(label='train', alpha=0.1, color='grey')\n",
        "series['test_ood']['target'][0].plot(label='test_ood')\n",
        "for f in series['test']['target']:\n",
        "    f.plot(label='test_id', color='orange', alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot 18h segments that start at 6AM\n",
        "fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        serie = series['train']['target'][i+3*j]\n",
        "        start = None\n",
        "        for k, hour in enumerate(serie.time_index.hour):\n",
        "            if hour == 6:\n",
        "                start = k\n",
        "                break\n",
        "        if start is not None and start + 216 <= len(serie):\n",
        "            serie = serie[start:(start+216)]\n",
        "            serie.plot(ax=axs[i, j], label='Segment {}'.format(i+3*j))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot 18h segments that start at 6AM\n",
        "fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        serie = series['train']['target'][i+3*j]\n",
        "        start = None\n",
        "        for k, hour in enumerate(serie.time_index.hour):\n",
        "            if hour == 6:\n",
        "                start = k\n",
        "                break\n",
        "        if start is not None and start + 216 <= len(serie):\n",
        "            serie = serie[start:(start+216)]\n",
        "            # rescale\n",
        "            serie = scalers['target'].inverse_transform(serie)\n",
        "            # set y-axis limits for axs[i, j]\n",
        "            serie.plot(ax=axs[i, j], label='Segment {}'.format(i+3*j))\n",
        "            axs[i, j].set_ylim(0, 420)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(test_data, scaler, in_len, out_len, stride, target_col, group_col):\n",
        "    errors = []\n",
        "    for group, data in test_data.groupby(group_col):\n",
        "        train_set = data[target_col].iloc[:in_len].values.flatten()\n",
        "        # fit model\n",
        "        model = AutoARIMA(start_p = 0,\n",
        "                        max_p = 10,\n",
        "                        start_q = 0,\n",
        "                        max_q = 10,\n",
        "                        start_P = 0,\n",
        "                        max_P = 10,\n",
        "                        start_Q=0,\n",
        "                        max_Q=10,\n",
        "                        allowdrift=True,\n",
        "                        allowmean=True,\n",
        "                        parallel=False)\n",
        "        model.fit(train_set)\n",
        "        # get valid sampling locations for future prediction\n",
        "        start_idx = np.arange(start=stride, stop=len(data) - in_len - out_len + 1, step=stride)\n",
        "        end_idx = start_idx + in_len\n",
        "        # iterate and collect predictions\n",
        "        for i in range(len(start_idx)):\n",
        "            input = data[target_col].iloc[start_idx[i]:end_idx[i]].values.flatten()\n",
        "            true = data[target_col].iloc[end_idx[i]:(end_idx[i]+out_len)].values.flatten()\n",
        "            prediction = model.forward(input, h=out_len)['mean']\n",
        "            # unscale true and prediction\n",
        "            true = scaler.inverse_transform(true.reshape(-1, 1)).flatten()\n",
        "            prediction = scaler.inverse_transform(prediction.reshape(-1, 1)).flatten()\n",
        "            # collect errors\n",
        "            errors.append(np.array([np.mean((true - prediction)**2), np.mean(np.abs(true - prediction))]))\n",
        "    errors = np.vstack(errors)\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "with open('./config/colas.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "config['split_params']['random_state'] = 0\n",
        "config['scaling_params']['scaler'] = 'StandardScaler'\n",
        "formatter = DataFormatter(config, study_file = None)\n",
        "\n",
        "# set params\n",
        "in_len = formatter.params['max_length_input']\n",
        "out_len = formatter.params['length_pred']\n",
        "stride = formatter.params['length_pred'] // 2\n",
        "target_col = formatter.get_column('target')\n",
        "group_col = formatter.get_column('sid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)]\n",
        "test_data_ood = formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]\n",
        "\n",
        "# backtest on the ID test set\n",
        "id_errors_sample = test_model(test_data, formatter.scalers[target_col[0]], in_len, out_len, stride, target_col, group_col)\n",
        "id_errors_sample = np.vstack(id_errors_sample)\n",
        "id_error_stats_sample = compute_error_statistics(id_errors_sample)\n",
        "\n",
        "# backtest on the ood test set\n",
        "ood_errors_sample = test_model(test_data_ood, formatter.scalers[target_col[0]], in_len, out_len, stride, target_col, group_col)\n",
        "ood_errors_sample = np.vstack(ood_errors_sample)\n",
        "ood_errors_stats_sample = compute_error_statistics(ood_errors_sample)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.LinearRegressionModel(lags = 32,\n",
        "                                     output_chunk_length = formatter.params['length_pred'])\n",
        "\n",
        "model.fit(series['train']['target'],\n",
        "          max_samples_per_ts=None, \n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecasts = model.historical_forecasts(series['test']['target'],\n",
        "                                        forecast_horizon=formatter.params['length_pred'], \n",
        "                                        stride=formatter.params['length_pred'] // 2,\n",
        "                                        retrain=False,\n",
        "                                        verbose=True,\n",
        "                                        last_points_only=False,\n",
        "                                        start=formatter.params['max_length_input'])\n",
        "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
        "                                    forecasts,  \n",
        "                                    [metrics.mse, metrics.mae],\n",
        "                                    scalers['target'],\n",
        "                                    reduction=None)\n",
        "id_errors_sample = np.vstack(id_errors_sample)\n",
        "np.median(id_errors_sample, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecasts = model.historical_forecasts(series['test_ood']['target'],\n",
        "                                                forecast_horizon=formatter.params['length_pred'], \n",
        "                                                stride=formatter.params['length_pred'] // 2,\n",
        "                                                retrain=False,\n",
        "                                                verbose=True,\n",
        "                                                last_points_only=False,\n",
        "                                                start=formatter.params[\"max_length_input\"])\n",
        "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
        "                            forecasts,  \n",
        "                            [metrics.mse, metrics.mae],\n",
        "                            scalers['target'],\n",
        "                            reduction=None)\n",
        "ood_errors_sample = np.vstack(ood_errors_sample)\n",
        "np.median(ood_errors_sample, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_ts = scalers['target'].inverse_transform(series['test_ood']['target'][0])\n",
        "predicted_ts = [scalers['target'].inverse_transform(f) for f in forecasts]\n",
        "\n",
        "errors = [\n",
        "    [metric_f(target_ts, f) for metric_f in [metrics.mse, metrics.mae]]\n",
        "    for f in predicted_ts\n",
        "    if f.time_index.hour[0] > 6\n",
        "]\n",
        "\n",
        "errors = np.array(errors)\n",
        "errors.shape\n",
        "\n",
        "np.median(errors, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
        "for i in range(6):\n",
        "    for j, f in enumerate(forecasts[i][:5]):\n",
        "        f.plot(ax=axs[j, i], label='forecast')\n",
        "        series['val']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
        "        axs[j, i].legend(fontsize=14)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "cNSYQNqZFrjg",
        "outputId": "fcc9eee3-619d-4279-9dfb-71e05ed72817"
      },
      "outputs": [],
      "source": [
        "model = models.XGBModel(lags=96, \n",
        "                        learning_rate=0.773,\n",
        "                        subsample=0.8,\n",
        "                        min_child_weight=1.0,\n",
        "                        colsample_bytree=1.0,\n",
        "                        max_depth=6,\n",
        "                        gamma=0.5,\n",
        "                        reg_alpha=0.167,\n",
        "                        reg_lambda=0.229,\n",
        "                        n_estimators=352,\n",
        "                        model_seed=0)\n",
        "\n",
        "model.fit(series['train']['target'],\n",
        "          max_samples_per_ts=None, \n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecasts = model.historical_forecasts(series['test']['target'],\n",
        "                                        forecast_horizon=formatter.params['length_pred'], \n",
        "                                        stride=formatter.params['length_pred'] // 2,\n",
        "                                        retrain=False,\n",
        "                                        verbose=True,\n",
        "                                        last_points_only=False,\n",
        "                                        start=formatter.params['max_length_input'])\n",
        "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
        "                                    forecasts,  \n",
        "                                    [metrics.mse, metrics.mae],\n",
        "                                    scalers['target'],\n",
        "                                    reduction=None)\n",
        "id_errors_sample = np.vstack(id_errors_sample)\n",
        "np.median(id_errors_sample, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
        "hours = []\n",
        "for i in range(len(series['test']['target'])):\n",
        "    hours += list(series['test']['target'][i].time_index[formatter.params['max_length_input']: ].hour)\n",
        "axs[0].hist(hours, bins=36)\n",
        "\n",
        "hours = []\n",
        "for i in range(len(series['test_ood']['target'])):\n",
        "    hours += list(series['test_ood']['target'][i].time_index[formatter.params['max_length_input']: ].hour)\n",
        "axs[1].hist(hours, bins=36)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecasts = model.historical_forecasts(series['test_ood']['target'],\n",
        "                                                forecast_horizon=formatter.params['length_pred'], \n",
        "                                                stride=formatter.params['length_pred'] // 2,\n",
        "                                                retrain=False,\n",
        "                                                verbose=True,\n",
        "                                                last_points_only=False,\n",
        "                                                start=formatter.params[\"max_length_input\"])\n",
        "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
        "                            forecasts,  \n",
        "                            [metrics.mse, metrics.mae],\n",
        "                            scalers['target'],\n",
        "                            reduction=None)\n",
        "ood_errors_sample = np.vstack(ood_errors_sample)\n",
        "np.median(ood_errors_sample, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rescale_and_backtest(series: Union[TimeSeries, Sequence[TimeSeries]],\n",
        "                         forecasts: Union[TimeSeries, Sequence[TimeSeries]], \n",
        "                         metric: Union[\n",
        "                                    Callable[[TimeSeries, TimeSeries], float],\n",
        "                                    List[Callable[[TimeSeries, TimeSeries], float]],\n",
        "                                ], \n",
        "                         scaler: Callable[[TimeSeries], TimeSeries] = None,\n",
        "                         reduction: Union[Callable[[np.ndarray], float], None] = np.mean,\n",
        "                        ):\n",
        "    \"\"\"\n",
        "    Backtest the forecasts on the series.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    series\n",
        "        The target time series.\n",
        "    forecasts\n",
        "        The forecasts.\n",
        "    scaler\n",
        "        The scaler used to scale the series.\n",
        "    metric\n",
        "        The metric to use for backtesting.\n",
        "    reduction\n",
        "        The reduction to apply to the metric.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float or List[float] or List[List[float]]\n",
        "        The (sequence of) error score on a series, or list of list containing error scores for each\n",
        "        provided series and each sample.\n",
        "    \"\"\"\n",
        "    series = [series] if isinstance(series, TimeSeries) else series\n",
        "    if len(series) == 1:\n",
        "        forecasts = [forecasts]\n",
        "    if not isinstance(metric, list):\n",
        "        metric = [metric]\n",
        "\n",
        "    # reverse scaling, forecasts and true values, compute errors\n",
        "    backtest_list = []\n",
        "    for idx, target_ts in enumerate(series):\n",
        "        if scaler is not None:\n",
        "            target_ts = scaler.inverse_transform(target_ts)\n",
        "            predicted_ts = [scaler.inverse_transform(f) for f in forecasts[idx]]\n",
        "        errors = [\n",
        "            [metric_f(target_ts, f) for metric_f in metric]\n",
        "            if len(metric) > 1\n",
        "            else metric[0](target_ts, f)\n",
        "            for f in predicted_ts\n",
        "            if f.time_index.hour[0] > 6 and f.time_index.hour[0] < 18\n",
        "        ]\n",
        "        if reduction is None:\n",
        "            backtest_list.append(np.array(errors))\n",
        "        else:\n",
        "            backtest_list.append(reduction(np.array(errors), axis=0))\n",
        "    return backtest_list if len(backtest_list) > 1 else backtest_list[0]\n",
        "\n",
        "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
        "                            forecasts,  \n",
        "                            [metrics.mse, metrics.mae],\n",
        "                            scalers['target'],\n",
        "                            reduction=None)\n",
        "ood_errors_sample = np.vstack(ood_errors_sample)\n",
        "np.median(ood_errors_sample, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
        "for i in range(6):\n",
        "    for j, f in enumerate(forecasts[i][:5]):\n",
        "        true = scalers['target'].inverse_transform(series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36))\n",
        "        forecast = scalers['target'].inverse_transform(f)\n",
        "        forecast.plot(ax=axs[j, i], label='forecast')\n",
        "        true.plot(ax=axs[j, i], label='true')\n",
        "        axs[j, i].legend(fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
        "for i in range(6):\n",
        "    for j in range(5):\n",
        "        forecast = scalers['target'].inverse_transform(forecasts[i+6*j])\n",
        "        true = scalers['target'].inverse_transform(series['test_ood']['target'][0].slice_n_points_after(forecast.time_index[0] - pd.Timedelta(\"2h\"), 36))\n",
        "        forecast.plot(ax=axs[j, i], label='forecast')\n",
        "        true.plot(ax=axs[j, i], label='true')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "model_name = f'tensorboard_tft_weinstock'\n",
        "work_dir = './output'\n",
        "el_stopper = EarlyStopping(\n",
        "                            monitor=\"val_loss\",\n",
        "                            patience=20,\n",
        "                            min_delta=0.001,\n",
        "                            mode='min',\n",
        "                            )\n",
        "loss_logger = LossLogger()\n",
        "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
        "\n",
        "# build the TFTModel model\n",
        "model = models.TFTModel(input_chunk_length = 96, \n",
        "                        output_chunk_length = formatter.params['length_pred'], \n",
        "                        hidden_size = 64,\n",
        "                        lstm_layers = 1,\n",
        "                        num_attention_heads = 4,\n",
        "                        full_attention = True,\n",
        "                        dropout = 0.1,\n",
        "                        hidden_continuous_size = 16,\n",
        "                        add_relative_index = True,\n",
        "                        model_name = model_name,\n",
        "                        work_dir = work_dir,\n",
        "                        log_tensorboard = True,\n",
        "                        pl_trainer_kwargs = pl_trainer_kwargs,\n",
        "                        batch_size = 64,\n",
        "                        optimizer_kwargs = {'lr': 0.001},\n",
        "                        save_checkpoints = True,\n",
        "                        force_reset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(series=series['train']['target'],\n",
        "              val_series=series['val']['target'],\n",
        "              max_samples_per_ts=200,\n",
        "              verbose=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecasts = model.historical_forecasts(series['test']['target'],\n",
        "                                        forecast_horizon=formatter.params['length_pred'], \n",
        "                                        stride=formatter.params['length_pred'] // 2,\n",
        "                                        retrain=False,\n",
        "                                        verbose=False,\n",
        "                                        last_points_only=False,\n",
        "                                        start=formatter.params['max_length_input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "errors = rescale_and_backtest(series['test']['target'],\n",
        "                                      forecasts,  \n",
        "                                      [metrics.mse, metrics.mae],\n",
        "                                      scalers['target'],\n",
        "                                      reduction=None)\n",
        "errors = np.vstack(errors)         \n",
        "np.median(errors, axis=0)                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
        "for i in range(6):\n",
        "    for j, f in enumerate(forecasts[i][:5]):\n",
        "        f.plot(ax=axs[j, i], label='forecast')\n",
        "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
        "        axs[j, i].legend(fontsize=14)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fY8-Bno2Frjg"
      },
      "source": [
        "## NHiTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "model_name = f'tensorboard_nhits_weinstock'\n",
        "work_dir = './output'\n",
        "el_stopper = EarlyStopping(monitor=\"val_loss\", patience=20, min_delta=0.001, mode='min')\n",
        "loss_logger = LossLogger()\n",
        "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
        "\n",
        "# build the TFTModel model\n",
        "model = models.NHiTSModel(input_chunk_length=96, \n",
        "                            output_chunk_length=12, \n",
        "                            num_stacks=3, \n",
        "                            num_blocks=1, \n",
        "                            num_layers=2, \n",
        "                            layer_widths=512,  \n",
        "                            n_freq_downsample=None, \n",
        "                            dropout=0.05, \n",
        "                            activation='ReLU',\n",
        "                            log_tensorboard = True,\n",
        "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
        "                            batch_size = 64,\n",
        "                            optimizer_kwargs = {'lr': 0.001},\n",
        "                            save_checkpoints = True,\n",
        "                            model_name = model_name,\n",
        "                            work_dir = work_dir,\n",
        "                            force_reset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oEn41SUFrjh"
      },
      "outputs": [],
      "source": [
        "model.fit(series=series['train']['target'],\n",
        "              val_series=series['val']['target'],\n",
        "              max_samples_per_ts=200,\n",
        "              verbose=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9qd9YmrFrjh"
      },
      "outputs": [],
      "source": [
        "model.load_from_checkpoint(model_name, work_dir = work_dir)\n",
        "\n",
        "forecasts = model.historical_forecasts(series['test']['target'],\n",
        "                                        forecast_horizon=formatter.params['length_pred'], \n",
        "                                        stride=formatter.params['length_pred'] // 2,\n",
        "                                        retrain=False,\n",
        "                                        verbose=False,\n",
        "                                        last_points_only=False,\n",
        "                                        start=formatter.params['max_length_input'])\n",
        "\n",
        "errors = rescale_and_backtest(series['test']['target'],\n",
        "                                      forecasts,  \n",
        "                                      [metrics.mse, metrics.mae],\n",
        "                                      scalers['target'],\n",
        "                                      reduction=None)\n",
        "errors = np.vstack(errors)         \n",
        "np.median(errors, axis=0)                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vmBLBJRFrji",
        "outputId": "77395780-d9b2-4984-9b63-728740d0988d"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
        "for i in range(6):\n",
        "    for j, f in enumerate(forecasts[i][:5]):\n",
        "        f.plot(ax=axs[j, i], label='forecast')\n",
        "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
        "        axs[j, i].legend(fontsize=14)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zp-vyrWFrjj",
        "outputId": "7267d782-f7bd-4308-b145-d7e085e11527"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "model_name = f'tensorboard_transformer_weinstock'\n",
        "work_dir = './output'\n",
        "el_stopper = EarlyStopping(monitor=\"val_loss\", patience=20, min_delta=0.001, mode='min')\n",
        "loss_logger = LossLogger()\n",
        "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
        "\n",
        "# build the TFTModel model\n",
        "model = models.TransformerModel(input_chunk_length=96, \n",
        "                            output_chunk_length=12, \n",
        "                            d_model=64, \n",
        "                            nhead=4, \n",
        "                            num_encoder_layers=3, \n",
        "                            num_decoder_layers=3, \n",
        "                            dim_feedforward=512, \n",
        "                            dropout=0.1,\n",
        "                            log_tensorboard = True,\n",
        "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
        "                            batch_size = 64,\n",
        "                            optimizer_kwargs = {'lr': 0.001},\n",
        "                            save_checkpoints = True,\n",
        "                            model_name = model_name,\n",
        "                            work_dir = work_dir,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8yhe79uFrjk"
      },
      "outputs": [],
      "source": [
        "model.fit(series=series['train']['target'],\n",
        "              val_series=series['val']['target'],\n",
        "              max_samples_per_ts=200,\n",
        "              verbose=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_from_checkpoint(model_name, work_dir = work_dir)\n",
        "\n",
        "forecasts = model.historical_forecasts(series['test']['target'],\n",
        "                                        forecast_horizon=formatter.params['length_pred'], \n",
        "                                        stride=formatter.params['length_pred'] // 2,\n",
        "                                        retrain=False,\n",
        "                                        verbose=False,\n",
        "                                        last_points_only=False,\n",
        "                                        start=formatter.params['max_length_input'])\n",
        "\n",
        "errors = rescale_and_backtest(series['test']['target'],\n",
        "                                      forecasts,  \n",
        "                                      [metrics.mse, metrics.mae],\n",
        "                                      scalers['target'],\n",
        "                                      reduction=None)\n",
        "errors = np.vstack(errors)         \n",
        "np.median(errors, axis=0)                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
        "for i in range(6):\n",
        "    for j, f in enumerate(forecasts[i][:5]):\n",
        "        f.plot(ax=axs[j, i], label='forecast')\n",
        "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
        "        axs[j, i].legend(fontsize=14)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model (with covariates)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert data and optional scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(seed = 0, study_file = None):\n",
        "    # load data\n",
        "    with open('./config/colas.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    config['split_params']['random_state'] = seed\n",
        "    formatter = DataFormatter(config, study_file = study_file)\n",
        "\n",
        "    # convert to series\n",
        "    time_col = formatter.get_column('time')\n",
        "    group_col = formatter.get_column('sid')\n",
        "    target_col = formatter.get_column('target')\n",
        "    static_cols = formatter.get_column('static_covs')\n",
        "    static_cols = static_cols + [formatter.get_column('id')] if static_cols is not None else [formatter.get_column('id')]\n",
        "    dynamic_cols = formatter.get_column('dynamic_covs')\n",
        "    future_cols = formatter.get_column('future_covs')\n",
        "\n",
        "    # build series\n",
        "    series, scalers = make_series({'train': formatter.train_data,\n",
        "                                    'val': formatter.val_data,\n",
        "                                    'test': formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)],\n",
        "                                    'test_ood': formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]},\n",
        "                                    time_col,\n",
        "                                    group_col,\n",
        "                                    {'target': target_col,\n",
        "                                    'static': static_cols,\n",
        "                                    'dynamic': dynamic_cols,\n",
        "                                    'future': future_cols})\n",
        "    \n",
        "    return formatter, series, scalers\n",
        "\n",
        "formatter, series, scalers = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id = formatter.get_column('id')\n",
        "time = formatter.get_column('time')\n",
        "target = formatter.get_column('target')[0]\n",
        "sid = formatter.get_column('sid')\n",
        "# plot for 9 random ids\n",
        "ids = np.random.choice(np.unique(formatter.train_data[id]), 9)\n",
        "fig, axs = plt.subplots(3, 3, figsize=(20, 10))\n",
        "for j, i in enumerate(ids):\n",
        "    train_data = formatter.train_data[formatter.train_data[id] == i]\n",
        "    val_data = None; test_data = None\n",
        "    # check that i is an id in the validation data\n",
        "    if i in np.unique(formatter.val_data[id]):\n",
        "        val_data = formatter.val_data[formatter.val_data[id] == i]\n",
        "    # check that i is an id in the test data\n",
        "    if i in np.unique(formatter.test_data[id]):\n",
        "        test_data = formatter.test_data[formatter.test_data[id] == i]\n",
        "    # plot the data using seaborn\n",
        "    sns.lineplot(x = time, y = target, data = train_data, hue=sid, ax=axs[j // 3, j % 3])\n",
        "    if val_data is not None:\n",
        "        sns.lineplot(x = time, y = target, data = val_data, color='orange', ax=axs[j // 3, j % 3])\n",
        "    if test_data is not None:\n",
        "        sns.lineplot(x = time, y = target, data = test_data, color='green', ax=axs[j // 3, j % 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "static_cols = formatter.get_column('static_covs')\n",
        "static_cols = static_cols + [formatter.get_column('id')] if static_cols is not None else [formatter.get_column('id')]\n",
        "dynamic_cols = formatter.get_column('dynamic_covs')\n",
        "future_cols = formatter.get_column('future_covs')\n",
        "# convert None to empty list\n",
        "static_cols = [] if static_cols is None else static_cols\n",
        "dynamic_cols = [] if dynamic_cols is None else dynamic_cols\n",
        "future_cols = [] if future_cols is None else future_cols\n",
        "# concatenate all covariates\n",
        "covs = static_cols + dynamic_cols + future_cols\n",
        "\n",
        "train_data = formatter.train_data\n",
        "test_data = formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)]\n",
        "test_data_ood = formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]\n",
        "\n",
        "# plot histograms and densities of covariates using seaborn\n",
        "fig, axs = plt.subplots(len(covs) // 3 + 1, 3, figsize=(10, 20))\n",
        "for i, c in enumerate(covs):\n",
        "    # create a dataframe with covariate and type\n",
        "    df = pd.DataFrame({'covariate': train_data[c].values, 'type': 'train'})\n",
        "    df2 = pd.DataFrame({'covariate': test_data[c].values, 'type': 'test'})\n",
        "    df3 = pd.DataFrame({'covariate': test_data_ood[c].values, 'type': 'test_ood'})\n",
        "    df = pd.concat([df, df2, df3])\n",
        "    # reset index to prevent errors\n",
        "    df = df.reset_index()\n",
        "    # plot density\n",
        "    sns.histplot(data=df, \n",
        "                 x='covariate', \n",
        "                 hue='type', \n",
        "                 ax=axs[i // 3, i % 3], \n",
        "                 stat='density', \n",
        "                 common_norm=False, \n",
        "                 common_bins=True,\n",
        "                 multiple='fill')\n",
        "    axs[i // 3, i % 3].set_title(c)\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(series['train']['future']))\n",
        "print(len(series['train']['target']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_segments = 5\n",
        "\n",
        "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
        "for i in range(5):\n",
        "    series['train']['target'][i].plot(ax = axs[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_segments = 5\n",
        "\n",
        "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
        "for i in range(5):\n",
        "    series['train']['future'][i].plot(ax = axs[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# attach static covariates to series\n",
        "for i in range(len(series['train']['target'])):\n",
        "    static_covs = series['train']['static'][i][0].pd_dataframe()\n",
        "    series['train']['target'][i] = series['train']['target'][i].with_static_covariates(static_covs)\n",
        "# attach to validation and test series\n",
        "for i in range(len(series['val']['target'])):\n",
        "    static_covs = series['val']['static'][i][0].pd_dataframe()\n",
        "    series['val']['target'][i] = series['val']['target'][i].with_static_covariates(static_covs)\n",
        "for i in range(len(series['test']['target'])):\n",
        "    static_covs = series['test']['static'][i][0].pd_dataframe()\n",
        "    series['test']['target'][i] = series['test']['target'][i].with_static_covariates(static_covs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "series['train']['target'][9]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.LinearRegressionModel(lags = 60,\n",
        "                                     output_chunk_length = formatter.params['length_pred'])\n",
        "\n",
        "model.fit(series['train']['target'],\n",
        "          max_samples_per_ts=100, \n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecasts = model.historical_forecasts(series['val']['target'],\n",
        "                                        forecast_horizon=formatter.params['length_pred'], \n",
        "                                        stride=formatter.params['length_pred'] // 2,\n",
        "                                        retrain=False,\n",
        "                                        verbose=True,\n",
        "                                        last_points_only=False,\n",
        "                                        start=formatter.params['max_length_input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "errors_sample = rescale_and_backtest(series['val']['target'],\n",
        "                                          forecasts,  \n",
        "                                          [metrics.mse, metrics.mae],\n",
        "                                          scalers['target'],\n",
        "                                          reduction=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.median(np.vstack(id_errors_sample), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
        "for i in range(6):\n",
        "    for j, f in enumerate(forecasts[i][:5]):\n",
        "        f.plot(ax=axs[j, i], label='forecast')\n",
        "        series['val']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
        "        axs[j, i].legend(fontsize=14)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

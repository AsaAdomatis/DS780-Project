{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "sys.path.insert(1, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "import optuna\n",
    "\n",
    "from darts import models\n",
    "from darts import metrics\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "from data_formatter.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Loading column definition...\n",
      "Checking column definition...\n",
      "Loading data...\n",
      "Checking for NA values...\n",
      "Setting data types...\n",
      "Dropping columns / rows...\n",
      "Encoding data...\n",
      "\tUpdated column definition:\n",
      "\t\tid: REAL_VALUED (ID)\n",
      "\t\ttime: DATE (TIME)\n",
      "\t\tgl: REAL_VALUED (TARGET)\n",
      "\t\ttime_year: REAL_VALUED (KNOWN_INPUT)\n",
      "\t\ttime_month: REAL_VALUED (KNOWN_INPUT)\n",
      "\t\ttime_day: REAL_VALUED (KNOWN_INPUT)\n",
      "\t\ttime_hour: REAL_VALUED (KNOWN_INPUT)\n",
      "\t\ttime_minute: REAL_VALUED (KNOWN_INPUT)\n",
      "Interpolating data...\n",
      "\tDropped segments: 1443\n",
      "\tExtracted segments: 654\n",
      "Splitting data...\n",
      "\tTrain: 572926 (100.00%)\n",
      "\tVal: 0 (0.00%)\n",
      "\tTest: 0 (0.00%)\n",
      "Scaling data...\n",
      "\tNo scaling applied\n",
      "Data formatting complete.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load yaml config file\n",
    "with open('./config/weinstock.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# set interpolation params for no interpolation\n",
    "new_config = config.copy()\n",
    "new_config['interpolation_params']['gap_threshold'] = 5\n",
    "new_config['interpolation_params']['min_drop_length'] = 0\n",
    "# set split params for no splitting\n",
    "new_config['split_params']['test_percent_subjects'] = 0\n",
    "new_config['split_params']['length_segment'] = 0\n",
    "# set scaling params for no scaling\n",
    "new_config['scaling_params']['scaler'] = 'None'\n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 9 segments\n",
    "num_segments = len(formatter.train_data['id_segment'].unique()[0:9])\n",
    "plot_data = formatter.train_data.loc[formatter.train_data['id_segment'].between(0,8)]\n",
    "\n",
    "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50% of the segments length is less than 6 that is extremely low. Hence, the interpolation is strongly needed and some very short segments should be dropped as well.\n",
    "\n",
    "First, use gap threshold as 45 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set interpolation params for interpolation\n",
    "new_config['interpolation_params']['gap_threshold'] = 45 # minutes - use as in config file \n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\t3rd Quartile: ', np.quantile(segment_lens, 0.75))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 9 segments\n",
    "num_segments = len(formatter.train_data['id_segment'].unique()[0:9])\n",
    "plot_data = formatter.train_data.loc[formatter.train_data['id_segment'].between(0,8)]\n",
    "\n",
    "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolatolation helped partially, but 25% of the segments are still too short - their length is less than 25 measurements, and half of the segments - less than 120. The range of the segments length is too big. There is more than three times difference between median and third quartile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of random samples from first 9 segments segments\n",
    "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
    "lags = 300\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    # select 10 random samples from index of data\n",
    "    sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "    # plot acf / pacf of each sample\n",
    "    for j in sample:\n",
    "        acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "        pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "        ax[0, i].plot(acf)\n",
    "        ax[1, i].plot(pacf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ACF & PACF plots from the above we can probably distinguish segments that were affected by interpolation - their ACF and PACF plots are much more smooth and have almost no variance between samples from index of data. \n",
    "\n",
    "To get the right idea about which threshold to put on the segments length to drop, more ACF & PACF plots should be analyzed. Below are the plots for nine segments with the biggest length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ids of the nine segments with biggest length\n",
    "inx = np.array(formatter.train_data.groupby('id_segment').size().sort_values(ascending=False).iloc[:9,].index)\n",
    "# sebset the data by these ids\n",
    "plot_data_max = formatter.train_data.loc[formatter.train_data['id_segment'].isin(inx)]\n",
    "num_segments_max = len(inx)\n",
    "\n",
    "# plot acf of random samples from segments\n",
    "fig, ax = plt.subplots(2, num_segments_max, figsize=(30, 5))\n",
    "lags = 300\n",
    "for i, (group, data) in enumerate(plot_data_max.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    # select 10 random samples from index of data\n",
    "    sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "    # plot acf / pacf of each sample\n",
    "    for j in sample:\n",
    "        acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "        pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "        ax[0, i].plot(acf)\n",
    "        ax[1, i].plot(pacf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to ACF plots, the correlation drops below 0.2 or so approximately after 200 lags. Hence, the segments with length less than, let's say, 250 will be dropped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set interpolation params to drop segments\n",
    "new_config['interpolation_params']['min_drop_length'] = 250\n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 16 segments\n",
    "num_segments = len(formatter.train_data['id_segment'].unique()[0:16])\n",
    "plot_data = formatter.train_data.loc[formatter.train_data['id_segment'].isin(formatter.train_data['id_segment'].unique()[0:16])]\n",
    "\n",
    "fig, axs = plt.subplots(2, num_segments//2, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    if i <= 7:\n",
    "        data.plot(x='time', y='gl', ax=axs[0,i], title='Segment {}'.format(group))\n",
    "    else:\n",
    "        data.plot(x='time', y='gl', ax=axs[1,i-8], title='Segment {}'.format(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of random samples from first 16 segments\n",
    "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
    "lags = 300\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    # select 10 random samples from index of data\n",
    "    sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "    # plot acf / pacf of each sample\n",
    "    for j in sample:\n",
    "        acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "        pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "        ax[0, i].plot(acf)\n",
    "        ax[1, i].plot(pacf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very hard to name the proper parameters for ARIMA model based on current ACF and PACF plots since within each segment, samples are behaving very differently showing different structures suitable for ARIMA model. However, we can still spot some common traits between segments. First, the autocorrelation graphs decays exponentially for almost every segment, on average, up to 20-50 lags (in some cases up to 100). Hence, the Auto Regression (AR) parameter can be set around these numbers. The partial autocorrelation plots pick around 2 for the first time and become close to zero after 5 lags at max. So, the Moving Average (MA) parameter can be set at 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ValueError: Could not correctly fill missing dates with the observed/passed frequency freq='5T'. Not all input time stamps contained in the newly created TimeSeries. For more information about frequency aliases, read https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not correctly fill missing dates with the observed/passed frequency freq='5T'. Not all input time stamps contained in the newly created TimeSeries. For more information about frequency aliases, read https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v0/mb03b1td28q2hxkbcqlcb7j00000gn/T/ipykernel_3571/806510237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgroup_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_series = TimeSeries.from_group_dataframe(formatter.train_data, \n\u001b[0m\u001b[1;32m      6\u001b[0m                                                \u001b[0mgroup_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                \u001b[0mtime_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/darts/timeseries.py\u001b[0m in \u001b[0;36mfrom_group_dataframe\u001b[0;34m(cls, df, group_cols, time_col, value_cols, static_cols, fill_missing_dates, freq, fillna_value)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# create a list with multiple TimeSeries and add static covariates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         return [\n\u001b[0m\u001b[1;32m    803\u001b[0m             TimeSeries.from_dataframe(\n\u001b[1;32m    804\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/darts/timeseries.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# create a list with multiple TimeSeries and add static covariates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         return [\n\u001b[0;32m--> 803\u001b[0;31m             TimeSeries.from_dataframe(\n\u001b[0m\u001b[1;32m    804\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0mtime_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/darts/timeseries.py\u001b[0m in \u001b[0;36mfrom_dataframe\u001b[0;34m(cls, df, time_col, value_cols, fill_missing_dates, freq, fillna_value, static_covariates, hierarchy)\u001b[0m\n\u001b[1;32m    685\u001b[0m         )\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m         return cls.from_xarray(\n\u001b[0m\u001b[1;32m    688\u001b[0m             \u001b[0mxa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mfill_missing_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_missing_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/darts/timeseries.py\u001b[0m in \u001b[0;36mfrom_xarray\u001b[0;34m(cls, xa, fill_missing_dates, freq, fillna_value)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# optionally fill missing dates; do it only when there is a DatetimeIndex (and not a RangeIndex)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfill_missing_dates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_datetime_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mxa_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_missing_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;31m# The provided index does not have a freq; using the provided freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhas_datetime_index\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_frequency\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/darts/timeseries.py\u001b[0m in \u001b[0;36m_fill_missing_dates\u001b[0;34m(cls, xa, freq)\u001b[0m\n\u001b[1;32m   4260\u001b[0m         \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobserved_freqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_xarray_from_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_xa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4264\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/darts/timeseries.py\u001b[0m in \u001b[0;36m_restore_xarray_from_frequency\u001b[0;34m(xa, freq)\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;34m\"https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4311\u001b[0m         )\n\u001b[0;32m-> 4312\u001b[0;31m         raise_if_not(\n\u001b[0m\u001b[1;32m   4313\u001b[0m             \u001b[0mcontains_all_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;34mf\"Could not correctly fill missing dates with the observed/passed frequency freq='{freq}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/darts/logging.py\u001b[0m in \u001b[0;36mraise_if_not\u001b[0;34m(condition, message, logger)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ValueError: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not correctly fill missing dates with the observed/passed frequency freq='5T'. Not all input time stamps contained in the newly created TimeSeries. For more information about frequency aliases, read https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
     ]
    }
   ],
   "source": [
    "# build target series\n",
    "target_col = formatter.get_column('target')\n",
    "time_col = formatter.get_column('time')\n",
    "group_col = formatter.get_column('sid')\n",
    "train_series = TimeSeries.from_group_dataframe(formatter.train_data, \n",
    "                                               group_cols = group_col, \n",
    "                                               time_col = time_col, \n",
    "                                               value_cols = target_col,\n",
    "                                              fill_missing_dates=True, freq=None)\n",
    "val_series = TimeSeries.from_group_dataframe(formatter.val_data,\n",
    "                                             group_cols = group_col,\n",
    "                                             time_col = time_col,\n",
    "                                             value_cols = target_col)\n",
    "test_series = TimeSeries.from_group_dataframe(formatter.test_data,\n",
    "                                              group_cols = group_col,\n",
    "                                              time_col = time_col,\n",
    "                                              value_cols = target_col)\n",
    "\n",
    "# build static covariates series\n",
    "static_cols = formatter.get_column('static_covs')\n",
    "if static_cols is not None:\n",
    "    static_cols += [formatter.get_column('id')]\n",
    "else:\n",
    "    static_cols = [formatter.get_column('id')]\n",
    "train_static = TimeSeries.from_group_dataframe(formatter.train_data, \n",
    "                                               group_cols = group_col, \n",
    "                                               time_col = time_col, \n",
    "                                               value_cols = static_cols)\n",
    "val_static = TimeSeries.from_group_dataframe(formatter.val_data,\n",
    "                                             group_cols = group_col,\n",
    "                                             time_col = time_col,\n",
    "                                             value_cols = static_cols)\n",
    "test_static = TimeSeries.from_group_dataframe(formatter.test_data,\n",
    "                                              group_cols = group_col,\n",
    "                                              time_col = time_col,\n",
    "                                              value_cols = static_cols)\n",
    "\n",
    "# build dynamic covariates series\n",
    "dynamic_cols = formatter.get_column('dynamic_covs')\n",
    "if dynamic_cols is not None:\n",
    "    train_dynamic = TimeSeries.from_group_dataframe(formatter.train_data, \n",
    "                                                    group_cols = group_col, \n",
    "                                                    time_col = time_col, \n",
    "                                                    value_cols = dynamic_cols)\n",
    "    val_dynamic = TimeSeries.from_group_dataframe(formatter.val_data,\n",
    "                                                  group_cols = group_col,\n",
    "                                                  time_col = time_col,\n",
    "                                                  value_cols = dynamic_cols)\n",
    "    test_dynamic = TimeSeries.from_group_dataframe(formatter.test_data,\n",
    "                                                   group_cols = group_col,\n",
    "                                                   time_col = time_col,\n",
    "                                                   value_cols = dynamic_cols)\n",
    "\n",
    "# build future covariates series\n",
    "future_cols = formatter.get_column('future_covs')\n",
    "if future_cols is not None:\n",
    "    train_future = TimeSeries.from_group_dataframe(formatter.train_data, \n",
    "                                                   group_cols = group_col, \n",
    "                                                   time_col = time_col, \n",
    "                                                   value_cols = future_cols)\n",
    "    val_future = TimeSeries.from_group_dataframe(formatter.val_data,\n",
    "                                                 group_cols = group_col,\n",
    "                                                 time_col = time_col,\n",
    "                                                 value_cols = future_cols)\n",
    "    test_future = TimeSeries.from_group_dataframe(formatter.test_data,\n",
    "                                                  group_cols = group_col,\n",
    "                                                  time_col = time_col,\n",
    "                                                  value_cols = future_cols)\n",
    "\n",
    "train_series[0].plot(label='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "58a3cc9cc47ada2c73d89b42b020767eadf9b8864b123d3efb345738bfde13c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

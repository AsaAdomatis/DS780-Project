{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc tools\n",
    "from typing import List, Union, Dict\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import warnings\n",
    "sys.path.insert(1, '..')\n",
    "os.chdir('..')\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "# analysis tools for time series\n",
    "import statsmodels.api as sm\n",
    "from statsforecast.models import AutoARIMA\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# darts\n",
    "from darts import models\n",
    "from darts import metrics\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "# utils for darts\n",
    "from data_formatter.base import *\n",
    "from utils.darts_dataset import *\n",
    "from utils.darts_processing import *\n",
    "from utils.darts_training import *\n",
    "from utils.darts_evaluation import *\n",
    "# gluformer model\n",
    "from lib.gluformer.model import Gluformer\n",
    "from lib.gluformer.utils.evaluation import test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glucose data\n",
    "df = pd.read_csv('raw_data/Weinstock2016_allfiles/Data Tables/BDataCGM.txt', sep='|')\n",
    "# create Pandas start date and add days from DeviceDaysFromEnroll column\n",
    "for id, data in df.groupby('PtID'):\n",
    "    dates = pd.to_datetime('1900-01-01') + pd.to_timedelta(data['DeviceDaysFromEnroll'], unit='d')\n",
    "    df.loc[data.index, 'Date'] = dates\n",
    "# drop rows where glucose is NA\n",
    "df.dropna(inplace=True, subset='Glucose')\n",
    "# create full time column\n",
    "df['time'] =pd.to_datetime(df['Date'].astype(str) + ' ' + df['DeviceTm'])\n",
    "# rename Glucose column to gl and PtID to id\n",
    "df.rename(columns={'Glucose': 'gl', 'PtID': 'id'}, inplace=True)\n",
    "# drop all columns except id, time, and gl\n",
    "df.drop(columns=[col for col in df.columns if col not in ['gl', 'time', 'id']], inplace=True)\n",
    "# reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demographics data\n",
    "df_demo = pd.read_csv('raw_data/Weinstock2016_allfiles/Data Tables/BDemoLifeDiabHxMgmt.txt', sep='|')\n",
    "select_cols = ['PtID']\n",
    "# select gender\n",
    "select_cols.append('Gender')\n",
    "# select race\n",
    "select_cols.append('Race')\n",
    "# select Education Level\n",
    "select_cols.append('EduLevel')\n",
    "df_demo['EduLevel'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# convert to numeric based on the mapping \n",
    "# 'Unknown' = 0,\n",
    "# '7th or 8th Grade' = 1, \n",
    "# '9th Grade' = 2,\n",
    "# '11th Grade' = 3, \n",
    "# '12th Grade - no diploma' = 4,\n",
    "# 'High school graduate/diploma/GED' = 5,\n",
    "# # 'Some college but no degree' = 6, \n",
    "# 'Associate Degree' = 7,\n",
    "# # 'Professional Degree' = 8,  \n",
    "# 'Bachelor's Degree' = 9, \n",
    "# 'Master's Degree' = 10, \n",
    "# 'Doctorate Degree' = 11,\n",
    "df_demo['EduLevel'] = df_demo['EduLevel'].map({'Unknown': 0, \n",
    "                                               '7th or 8th Grade': 1, \n",
    "                                               '9th Grade': 2, \n",
    "                                               '11th Grade': 3, \n",
    "                                               '12th Grade - no diploma': 4, \n",
    "                                               'High school graduate/diploma/GED': 5, \n",
    "                                               'Some college but no degree': 6, \n",
    "                                               'Associate Degree': 7, \n",
    "                                               'Professional Degree': 8, \n",
    "                                               \"Bachelor's Degree\": 9, \n",
    "                                               \"Master's Degree\": 10, \n",
    "                                               \"Doctorate Degree\": 11})\n",
    "# select AnnualInc\n",
    "select_cols.append('AnnualInc')\n",
    "df_demo['AnnualInc'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# convert to numeric based on the mapping\n",
    "# 'Unknown' = 0,\n",
    "# 'Less than $25,000' = 1,\n",
    "# '$25,000 - $35,000' = 2, \n",
    "# '$35,000 - less than $50,000' = 3,\n",
    "# '$50,000 - less than $75,000' = 4,\n",
    "# '$75,000 - less than $100,000' = 5,\n",
    "# '$100,000 - less than $200,000' = 6\n",
    "# '$200,000 or more' = 7\n",
    "df_demo['AnnualInc'] = df_demo['AnnualInc'].map({'Unknown': 0,\n",
    "                                                 'Less than $25,000': 1,\n",
    "                                                 '$25,000 - $35,000': 2,\n",
    "                                                 '$35,000 - less than $50,000': 3,\n",
    "                                                 '$50,000 - less than $75,000': 4,\n",
    "                                                 '$75,000 - less than $100,000': 5,\n",
    "                                                 '$100,000 - less than $200,000': 6,\n",
    "                                                 '$200,000 or more': 7})\n",
    "\n",
    "# select MaritalStatus\n",
    "select_cols.append('MaritalStatus')\n",
    "df_demo['MaritalStatus'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# select DaysWkEx\n",
    "select_cols.append('DaysWkEx')\n",
    "df_demo['DaysWkEx'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select DaysWkDrinkAlc\n",
    "select_cols.append('DaysWkDrinkAlc')\n",
    "df_demo['DaysWkDrinkAlc'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select DaysMonBingeAlc\n",
    "select_cols.append('DaysMonBingeAlc')\n",
    "df_demo['DaysMonBingeAlc'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select T1DDiagAge\n",
    "select_cols.append('T1DDiagAge')\n",
    "# select NumHospDKA\n",
    "select_cols.append('NumHospDKA')\n",
    "df_demo['NumHospDKA'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select NumSHSinceT1DDiag\n",
    "select_cols.append('NumSHSinceT1DDiag')\n",
    "# convert to numeric based on the mapping\n",
    "# '0' = 0, \n",
    "# '1' = 1\n",
    "# '2' = 2, \n",
    "# '3' = 3, \n",
    "# '4' = 4,\n",
    "# '5 - 9' = 5,\n",
    "# '10 - 19' = 6, \n",
    "# '>19' = 7\n",
    "df_demo['NumSHSinceT1DDiag'] = df_demo['NumSHSinceT1DDiag'].map({'0': 0,\n",
    "                                                                '1': 1,\n",
    "                                                                '2': 2,\n",
    "                                                                '3': 3,\n",
    "                                                                '4': 4,\n",
    "                                                                '5 - 9': 5,\n",
    "                                                                '10 - 19': 6,\n",
    "                                                                '>19': 7})\n",
    "# select InsDeliveryMethod\n",
    "select_cols.append('InsDeliveryMethod')\n",
    "# select UnitsInsTotal, replace NaN with 0\n",
    "select_cols.append('UnitsInsTotal')\n",
    "df_demo['UnitsInsTotal'].fillna(0, inplace=True)\n",
    "# select NumMeterCheckDay\n",
    "select_cols.append('NumMeterCheckDay')\n",
    "# convert to numeric based on the mapping\n",
    "# '0' = 0,\n",
    "# '1' = 1,\n",
    "# '2' = 2,\n",
    "# '3' = 3,\n",
    "# '4' = 4,\n",
    "# '5' = 5,\n",
    "# '6' = 6,\n",
    "# '7' = 7,\n",
    "# '8' = 8,\n",
    "# '9' = 9,\n",
    "# '10' = 10,\n",
    "# '11' = 11,\n",
    "# '12' = 12,\n",
    "# '13' = 13,\n",
    "# '14' = 14,\n",
    "# '15' = 15,\n",
    "# '16' = 16,\n",
    "# '17' = 17,\n",
    "# '18' = 18,\n",
    "# '> 19' = 19\n",
    "df_demo['NumMeterCheckDay'] = df_demo['NumMeterCheckDay'].map({'0': 0,\n",
    "                                                                '1': 1,\n",
    "                                                                '2': 2,\n",
    "                                                                '3': 3,\n",
    "                                                                '4': 4,\n",
    "                                                                '5': 5,\n",
    "                                                                '6': 6,\n",
    "                                                                '7': 7,\n",
    "                                                                '8': 8,\n",
    "                                                                '9': 9,\n",
    "                                                                '10': 10,\n",
    "                                                                '11': 11,\n",
    "                                                                '12': 12,\n",
    "                                                                '13': 13,\n",
    "                                                                '14': 14,\n",
    "                                                                '15': 15,\n",
    "                                                                '16': 16,\n",
    "                                                                '17': 17,\n",
    "                                                                '18': 18,\n",
    "                                                                '> 19': 19})\n",
    "# leave only selected columns\n",
    "df_demo = df_demo[select_cols]\n",
    "# rename PtID to id\n",
    "df_demo.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# print selected columns\n",
    "print(df_demo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medical conditions data\n",
    "df_medchart = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedChart.txt', sep='|')\n",
    "# convert weight to lbs\n",
    "df_medchart.loc[df_medchart['WeightUnits'] == 'kg', 'Weight'] = df_medchart.loc[df_medchart['WeightUnits'] == 'kg', 'Weight'] * 2.20462\n",
    "# convert height to inches\n",
    "df_medchart.loc[df_medchart['HeightUnits'] == 'cm', 'Height'] = df_medchart.loc[df_medchart['HeightUnits'] == 'cm', 'Height'] * 0.393701\n",
    "# select Height and Weight and fill NaN with 0\n",
    "df_medchart['Height'].fillna(0, inplace=True)\n",
    "df_medchart['Weight'].fillna(0, inplace=True)\n",
    "df_medchart = df_medchart[['PtID', 'Height', 'Weight']]\n",
    "# rename PtID to id\n",
    "df_medchart.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# print selected columns\n",
    "print(df_medchart.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medical conditions data\n",
    "df_medcond = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedicalConditions.txt', sep='|')\n",
    "# select top-13 illnesses (>10% of 201 patients have at least one of them based on value counts)\n",
    "top13_illnesses = df_medcond['MCLLTReal'].value_counts().index[:13]\n",
    "# create a one-hot encoding of the top-13 illnesses\n",
    "df_medcond = pd.get_dummies(df_medcond, columns=['MCLLTReal'], prefix='', prefix_sep='', dummy_na=True)\n",
    "df_medcond = df_medcond[['PtID'] + top13_illnesses.tolist()]\n",
    "# remove zero rows\n",
    "df_medcond = df_medcond.loc[(df_medcond[top13_illnesses] != 0).any(axis=1)]\n",
    "# rename PtID to id\n",
    "df_medcond.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# sum rows for the same id\n",
    "df_medcond = df_medcond.groupby('id').sum()\n",
    "# reset index\n",
    "df_medcond.reset_index(inplace=True)\n",
    "# print top-13 illnesses\n",
    "print(top13_illnesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medication data\n",
    "df_med = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedication.txt', sep='|')\n",
    "# select top-9 medications (>10% of 201 patients have at least one of them based on value counts)\n",
    "all_meds = df_med['DrugName'].unique()\n",
    "top9_meds = df_med['DrugName'].value_counts().index[:9]\n",
    "# create a one-hot encoding of the top-9 medications\n",
    "df_med = pd.get_dummies(df_med, columns=['DrugName'], prefix='', prefix_sep='', dummy_na=True)\n",
    "\n",
    "# strip first number from MedDose\n",
    "import re\n",
    "def strip_first_number(x):\n",
    "    x = str(x)\n",
    "    # remove all ,\n",
    "    x = x.replace(',', '')\n",
    "    # find first non-number and not . or , character in string x\n",
    "    first_non_num = re.search(r'[^0-9.]', x)\n",
    "    if first_non_num is None:\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float(x[:first_non_num.start()]) if first_non_num.start() > 0 else 1.0\n",
    "# apply strip_first_number to MedDose per element\n",
    "df_med['MedDose'].fillna(1, inplace=True)\n",
    "for i in range(len(df_med)):\n",
    "    df_med['MedDose'].iloc[i] = strip_first_number(df_med['MedDose'].iloc[i])\n",
    "\n",
    "# for each patient get the dose for each medication\n",
    "df_med[all_meds].values[df_med[all_meds] != 0] = df_med['MedDose']\n",
    "# select PtID and top-9 medications\n",
    "df_med = df_med[['PtID'] + top9_meds.tolist()]\n",
    "# remove zero rows\n",
    "df_med = df_med.loc[(df_med[top9_meds] != 0).any(axis=1)]\n",
    "# rename PtID to id\n",
    "df_med.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# sum rows for the same id\n",
    "df_med = df_med.groupby('id').sum()\n",
    "# reset index\n",
    "df_med.reset_index(inplace=True)\n",
    "# print top-9 medications\n",
    "print(top9_meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dataframes\n",
    "df_new = df.merge(df_demo, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_medchart, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_medcond, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_med, on = 'id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na values with zeros for df_med and df_medcond columns\n",
    "df_new[top13_illnesses] = df_new[top13_illnesses].fillna(0)\n",
    "df_new[top9_meds] = df_new[top9_meds].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as Weinstock2016_processed.csv\n",
    "df_new.to_csv('./raw_data/weinstock.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml config file\n",
    "with open('./config/weinstock.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# set interpolation params for no interpolation\n",
    "new_config = config.copy()\n",
    "new_config['interpolation_params']['gap_threshold'] = 5\n",
    "new_config['interpolation_params']['min_drop_length'] = 0\n",
    "# set split params for no splitting\n",
    "new_config['split_params']['test_percent_subjects'] = 0\n",
    "new_config['split_params']['length_segment'] = 0\n",
    "# set scaling params for no scaling\n",
    "new_config['scaling_params']['scaler'] = 'None'\n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 9 segments\n",
    "num_segments = 9\n",
    "plot_data = formatter.train_data\n",
    "\n",
    "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))\n",
    "    if i >= num_segments - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of random samples from first 9 segments segments\n",
    "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
    "lags = 300; k = 0\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    else:\n",
    "        # select 10 random samples from index of data\n",
    "        sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "        # plot acf / pacf of each sample\n",
    "        for j in sample:\n",
    "            acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "            pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "            ax[0, k].plot(acf)\n",
    "            ax[1, k].plot(pacf)\n",
    "        k += 1\n",
    "        if k >= num_segments:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF plots suggest that significant dependency persists up to 200 points (~16 hours). The analysis of distribution of segment lengths suggests that there are too many short segments. \n",
    "Based on this, interpolation should be performed of missing values up to 45 minutes (9 points), segments less than 200 points should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set interpolation params for interpolation\n",
    "new_config['interpolation_params']['gap_threshold'] = 45 # minutes - use as in config file \n",
    "new_config['interpolation_params']['min_drop_length'] = 240\n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\t3rd Quartile: ', np.quantile(segment_lens, 0.75))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 9 segments\n",
    "num_segments = 9\n",
    "plot_data = formatter.train_data\n",
    "\n",
    "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))\n",
    "    if i >= num_segments - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of random samples from first 9 segments segments\n",
    "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
    "lags = 300; k = 0\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    else:\n",
    "        # select 10 random samples from index of data\n",
    "        sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "        # plot acf / pacf of each sample\n",
    "        for j in sample:\n",
    "            acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "            pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "            ax[0, k].plot(acf)\n",
    "            ax[1, k].plot(pacf)\n",
    "        k += 1\n",
    "        if k >= num_segments:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very hard to name the proper parameters for ARIMA model based on current ACF and PACF plots since within each segment, samples are behaving very differently showing different structures suitable for ARIMA model. However, we can still spot some common traits between segments. First, the autocorrelation graphs decays exponentially for almost every segment, on average, up to 20-50 lags (in some cases up to 100). Hence, the Auto Regression (AR) parameter can be set around these numbers. The partial autocorrelation plots pick around 2 for the first time and become close to zero after 5 lags at max. So, the Moving Average (MA) parameter can be set at 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the config file according to the analysis above\n",
    "with open('./config/weinstock.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "# set interpolation params for no interpolation\n",
    "config['interpolation_params']['gap_threshold'] = 45\n",
    "config['interpolation_params']['min_drop_length'] = 240\n",
    "# set split params for no splitting\n",
    "config['split_params']['test_percent_subjects'] = 0.1\n",
    "config['split_params']['length_segment'] = 240\n",
    "# set scaling params for no scaling\n",
    "config['scaling_params']['scaler'] = 'None'\n",
    "\n",
    "formatter = DataFormatter(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9af0babfa4fcc32151d0f9cd96f26ee8eefb724c47cfe9b27c84c1db30f6822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

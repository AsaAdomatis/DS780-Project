{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import warnings\n",
    "sys.path.insert(1, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "import optuna\n",
    "import darts\n",
    "\n",
    "from darts import models\n",
    "from darts import metrics\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "from statsforecast.models import AutoARIMA\n",
    "\n",
    "from data_formatter.base import *\n",
    "from bin.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing raw data and adding covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glucose data\n",
    "df = pd.read_csv('raw_data/Weinstock2016_allfiles/Data Tables/BDataCGM.txt', sep='|')\n",
    "# create Pandas start date and add days from DeviceDaysFromEnroll column\n",
    "for id, data in df.groupby('PtID'):\n",
    "    dates = pd.to_datetime('1900-01-01') + pd.to_timedelta(data['DeviceDaysFromEnroll'], unit='d')\n",
    "    df.loc[data.index, 'Date'] = dates\n",
    "# drop rows where glucose is NA\n",
    "df.dropna(inplace=True, subset='Glucose')\n",
    "# create full time column\n",
    "df['time'] =pd.to_datetime(df['Date'].astype(str) + ' ' + df['DeviceTm'])\n",
    "# rename Glucose column to gl and PtID to id\n",
    "df.rename(columns={'Glucose': 'gl', 'PtID': 'id'}, inplace=True)\n",
    "# drop all columns except id, time, and gl\n",
    "df.drop(columns=[col for col in df.columns if col not in ['gl', 'time', 'id']], inplace=True)\n",
    "# reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demographics data\n",
    "df_demo = pd.read_csv('raw_data/Weinstock2016_allfiles/Data Tables/BDemoLifeDiabHxMgmt.txt', sep='|')\n",
    "select_cols = ['PtID']\n",
    "# select gender\n",
    "select_cols.append('Gender')\n",
    "# select race\n",
    "select_cols.append('Race')\n",
    "# select Education Level\n",
    "select_cols.append('EduLevel')\n",
    "df_demo['EduLevel'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# convert to numeric based on the mapping \n",
    "# 'Unknown' = 0,\n",
    "# '7th or 8th Grade' = 1, \n",
    "# '9th Grade' = 2,\n",
    "# '11th Grade' = 3, \n",
    "# '12th Grade - no diploma' = 4,\n",
    "# 'High school graduate/diploma/GED' = 5,\n",
    "# # 'Some college but no degree' = 6, \n",
    "# 'Associate Degree' = 7,\n",
    "# # 'Professional Degree' = 8,  \n",
    "# 'Bachelor's Degree' = 9, \n",
    "# 'Master's Degree' = 10, \n",
    "# 'Doctorate Degree' = 11,\n",
    "df_demo['EduLevel'] = df_demo['EduLevel'].map({'Unknown': 0, \n",
    "                                               '7th or 8th Grade': 1, \n",
    "                                               '9th Grade': 2, \n",
    "                                               '11th Grade': 3, \n",
    "                                               '12th Grade - no diploma': 4, \n",
    "                                               'High school graduate/diploma/GED': 5, \n",
    "                                               'Some college but no degree': 6, \n",
    "                                               'Associate Degree': 7, \n",
    "                                               'Professional Degree': 8, \n",
    "                                               \"Bachelor's Degree\": 9, \n",
    "                                               \"Master's Degree\": 10, \n",
    "                                               \"Doctorate Degree\": 11})\n",
    "# select AnnualInc\n",
    "select_cols.append('AnnualInc')\n",
    "df_demo['AnnualInc'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# convert to numeric based on the mapping\n",
    "# 'Unknown' = 0,\n",
    "# 'Less than $25,000' = 1,\n",
    "# '$25,000 - $35,000' = 2, \n",
    "# '$35,000 - less than $50,000' = 3,\n",
    "# '$50,000 - less than $75,000' = 4,\n",
    "# '$75,000 - less than $100,000' = 5,\n",
    "# '$100,000 - less than $200,000' = 6\n",
    "# '$200,000 or more' = 7\n",
    "df_demo['AnnualInc'] = df_demo['AnnualInc'].map({'Unknown': 0,\n",
    "                                                 'Less than $25,000': 1,\n",
    "                                                 '$25,000 - $35,000': 2,\n",
    "                                                 '$35,000 - less than $50,000': 3,\n",
    "                                                 '$50,000 - less than $75,000': 4,\n",
    "                                                 '$75,000 - less than $100,000': 5,\n",
    "                                                 '$100,000 - less than $200,000': 6,\n",
    "                                                 '$200,000 or more': 7})\n",
    "\n",
    "# select MaritalStatus\n",
    "select_cols.append('MaritalStatus')\n",
    "df_demo['MaritalStatus'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# select DaysWkEx\n",
    "select_cols.append('DaysWkEx')\n",
    "df_demo['DaysWkEx'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select DaysWkDrinkAlc\n",
    "select_cols.append('DaysWkDrinkAlc')\n",
    "df_demo['DaysWkDrinkAlc'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select DaysMonBingeAlc\n",
    "select_cols.append('DaysMonBingeAlc')\n",
    "df_demo['DaysMonBingeAlc'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select T1DDiagAge\n",
    "select_cols.append('T1DDiagAge')\n",
    "# select NumHospDKA\n",
    "select_cols.append('NumHospDKA')\n",
    "df_demo['NumHospDKA'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select NumSHSinceT1DDiag\n",
    "select_cols.append('NumSHSinceT1DDiag')\n",
    "# convert to numeric based on the mapping\n",
    "# '0' = 0, \n",
    "# '1' = 1\n",
    "# '2' = 2, \n",
    "# '3' = 3, \n",
    "# '4' = 4,\n",
    "# '5 - 9' = 5,\n",
    "# '10 - 19' = 6, \n",
    "# '>19' = 7\n",
    "df_demo['NumSHSinceT1DDiag'] = df_demo['NumSHSinceT1DDiag'].map({'0': 0,\n",
    "                                                                '1': 1,\n",
    "                                                                '2': 2,\n",
    "                                                                '3': 3,\n",
    "                                                                '4': 4,\n",
    "                                                                '5 - 9': 5,\n",
    "                                                                '10 - 19': 6,\n",
    "                                                                '>19': 7})\n",
    "# select InsDeliveryMethod\n",
    "select_cols.append('InsDeliveryMethod')\n",
    "# select UnitsInsTotal, replace NaN with 0\n",
    "select_cols.append('UnitsInsTotal')\n",
    "df_demo['UnitsInsTotal'].fillna(0, inplace=True)\n",
    "# select NumMeterCheckDay\n",
    "select_cols.append('NumMeterCheckDay')\n",
    "# convert to numeric based on the mapping\n",
    "# '0' = 0,\n",
    "# '1' = 1,\n",
    "# '2' = 2,\n",
    "# '3' = 3,\n",
    "# '4' = 4,\n",
    "# '5' = 5,\n",
    "# '6' = 6,\n",
    "# '7' = 7,\n",
    "# '8' = 8,\n",
    "# '9' = 9,\n",
    "# '10' = 10,\n",
    "# '11' = 11,\n",
    "# '12' = 12,\n",
    "# '13' = 13,\n",
    "# '14' = 14,\n",
    "# '15' = 15,\n",
    "# '16' = 16,\n",
    "# '17' = 17,\n",
    "# '18' = 18,\n",
    "# '> 19' = 19\n",
    "df_demo['NumMeterCheckDay'] = df_demo['NumMeterCheckDay'].map({'0': 0,\n",
    "                                                                '1': 1,\n",
    "                                                                '2': 2,\n",
    "                                                                '3': 3,\n",
    "                                                                '4': 4,\n",
    "                                                                '5': 5,\n",
    "                                                                '6': 6,\n",
    "                                                                '7': 7,\n",
    "                                                                '8': 8,\n",
    "                                                                '9': 9,\n",
    "                                                                '10': 10,\n",
    "                                                                '11': 11,\n",
    "                                                                '12': 12,\n",
    "                                                                '13': 13,\n",
    "                                                                '14': 14,\n",
    "                                                                '15': 15,\n",
    "                                                                '16': 16,\n",
    "                                                                '17': 17,\n",
    "                                                                '18': 18,\n",
    "                                                                '> 19': 19})\n",
    "# leave only selected columns\n",
    "df_demo = df_demo[select_cols]\n",
    "# rename PtID to id\n",
    "df_demo.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# print selected columns\n",
    "print(df_demo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medical conditions data\n",
    "df_medchart = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedChart.txt', sep='|')\n",
    "# convert weight to lbs\n",
    "df_medchart.loc[df_medchart['WeightUnits'] == 'kg', 'Weight'] = df_medchart.loc[df_medchart['WeightUnits'] == 'kg', 'Weight'] * 2.20462\n",
    "# convert height to inches\n",
    "df_medchart.loc[df_medchart['HeightUnits'] == 'cm', 'Height'] = df_medchart.loc[df_medchart['HeightUnits'] == 'cm', 'Height'] * 0.393701\n",
    "# select Height and Weight and fill NaN with 0\n",
    "df_medchart['Height'].fillna(0, inplace=True)\n",
    "df_medchart['Weight'].fillna(0, inplace=True)\n",
    "df_medchart = df_medchart[['PtID', 'Height', 'Weight']]\n",
    "# rename PtID to id\n",
    "df_medchart.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# print selected columns\n",
    "print(df_medchart.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medical conditions data\n",
    "df_medcond = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedicalConditions.txt', sep='|')\n",
    "# select top-13 illnesses (>10% of 201 patients have at least one of them based on value counts)\n",
    "top13_illnesses = df_medcond['MCLLTReal'].value_counts().index[:13]\n",
    "# create a one-hot encoding of the top-13 illnesses\n",
    "df_medcond = pd.get_dummies(df_medcond, columns=['MCLLTReal'], prefix='', prefix_sep='', dummy_na=True)\n",
    "df_medcond = df_medcond[['PtID'] + top13_illnesses.tolist()]\n",
    "# remove zero rows\n",
    "df_medcond = df_medcond.loc[(df_medcond[top13_illnesses] != 0).any(axis=1)]\n",
    "# rename PtID to id\n",
    "df_medcond.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# sum rows for the same id\n",
    "df_medcond = df_medcond.groupby('id').sum()\n",
    "# reset index\n",
    "df_medcond.reset_index(inplace=True)\n",
    "# print top-13 illnesses\n",
    "print(top13_illnesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medication data\n",
    "df_med = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedication.txt', sep='|')\n",
    "# select top-9 medications (>10% of 201 patients have at least one of them based on value counts)\n",
    "all_meds = df_med['DrugName'].unique()\n",
    "top9_meds = df_med['DrugName'].value_counts().index[:9]\n",
    "# create a one-hot encoding of the top-9 medications\n",
    "df_med = pd.get_dummies(df_med, columns=['DrugName'], prefix='', prefix_sep='', dummy_na=True)\n",
    "\n",
    "# strip first number from MedDose\n",
    "import re\n",
    "def strip_first_number(x):\n",
    "    x = str(x)\n",
    "    # remove all ,\n",
    "    x = x.replace(',', '')\n",
    "    # find first non-number and not . or , character in string x\n",
    "    first_non_num = re.search(r'[^0-9.]', x)\n",
    "    if first_non_num is None:\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float(x[:first_non_num.start()]) if first_non_num.start() > 0 else 1.0\n",
    "# apply strip_first_number to MedDose per element\n",
    "df_med['MedDose'].fillna(1, inplace=True)\n",
    "for i in range(len(df_med)):\n",
    "    df_med['MedDose'].iloc[i] = strip_first_number(df_med['MedDose'].iloc[i])\n",
    "\n",
    "# for each patient get the dose for each medication\n",
    "df_med[all_meds].values[df_med[all_meds] != 0] = df_med['MedDose']\n",
    "# select PtID and top-9 medications\n",
    "df_med = df_med[['PtID'] + top9_meds.tolist()]\n",
    "# remove zero rows\n",
    "df_med = df_med.loc[(df_med[top9_meds] != 0).any(axis=1)]\n",
    "# rename PtID to id\n",
    "df_med.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# sum rows for the same id\n",
    "df_med = df_med.groupby('id').sum()\n",
    "# reset index\n",
    "df_med.reset_index(inplace=True)\n",
    "# print top-9 medications\n",
    "print(top9_meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dataframes\n",
    "df_new = df.merge(df_demo, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_medchart, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_medcond, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_med, on = 'id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as Weinstock2016_processed.csv\n",
    "df_new.to_csv('./raw_data/Weinstock2016_processed_with_covariates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml config file\n",
    "with open('./config/weinstock.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# set interpolation params for no interpolation\n",
    "new_config = config.copy()\n",
    "new_config['interpolation_params']['gap_threshold'] = 5\n",
    "new_config['interpolation_params']['min_drop_length'] = 0\n",
    "# set split params for no splitting\n",
    "new_config['split_params']['test_percent_subjects'] = 0\n",
    "new_config['split_params']['length_segment'] = 0\n",
    "# set scaling params for no scaling\n",
    "new_config['scaling_params']['scaler'] = 'None'\n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 9 segments\n",
    "num_segments = 9\n",
    "plot_data = formatter.train_data\n",
    "\n",
    "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))\n",
    "    if i >= num_segments - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of random samples from first 9 segments segments\n",
    "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
    "lags = 300; k = 0\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    else:\n",
    "        # select 10 random samples from index of data\n",
    "        sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "        # plot acf / pacf of each sample\n",
    "        for j in sample:\n",
    "            acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "            pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "            ax[0, k].plot(acf)\n",
    "            ax[1, k].plot(pacf)\n",
    "        k += 1\n",
    "        if k >= num_segments:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF plots suggest that significant dependency persists up to 200 points (~16 hours). The analysis of distribution of segment lengths suggests that there are too many short segments. \n",
    "Based on this, interpolation should be performed of missing values up to 45 minutes (9 points), segments less than 200 points should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set interpolation params for interpolation\n",
    "new_config['interpolation_params']['gap_threshold'] = 45 # minutes - use as in config file \n",
    "new_config['interpolation_params']['min_drop_length'] = 240\n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\t3rd Quartile: ', np.quantile(segment_lens, 0.75))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 9 segments\n",
    "num_segments = 9\n",
    "plot_data = formatter.train_data\n",
    "\n",
    "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))\n",
    "    if i >= num_segments - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of random samples from first 9 segments segments\n",
    "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
    "lags = 300; k = 0\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    else:\n",
    "        # select 10 random samples from index of data\n",
    "        sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "        # plot acf / pacf of each sample\n",
    "        for j in sample:\n",
    "            acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "            pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "            ax[0, k].plot(acf)\n",
    "            ax[1, k].plot(pacf)\n",
    "        k += 1\n",
    "        if k >= num_segments:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very hard to name the proper parameters for ARIMA model based on current ACF and PACF plots since within each segment, samples are behaving very differently showing different structures suitable for ARIMA model. However, we can still spot some common traits between segments. First, the autocorrelation graphs decays exponentially for almost every segment, on average, up to 20-50 lags (in some cases up to 100). Hence, the Auto Regression (AR) parameter can be set around these numbers. The partial autocorrelation plots pick around 2 for the first time and become close to zero after 5 lags at max. So, the Moving Average (MA) parameter can be set at 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the config according to the observations above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config/weinstock.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "# set interpolation params for no interpolation\n",
    "config['interpolation_params']['gap_threshold'] = 45\n",
    "config['interpolation_params']['min_drop_length'] = 240\n",
    "# set split params for no splitting\n",
    "config['split_params']['test_percent_subjects'] = 0.1\n",
    "config['split_params']['length_segment'] = 240\n",
    "# set scaling params for no scaling\n",
    "config['scaling_params']['scaler'] = 'None'\n",
    "\n",
    "formatter = DataFormatter(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models (no covariates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data and (optional) scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Loading column definition...\n",
      "Checking column definition...\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/m/mrsergazinov/GluNet/data_formatter/base.py:51: DtypeWarning: Columns (19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(self.params['data_csv_path'], index_col=self.params['index_col'], na_filter=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NA values...\n",
      "Setting data types...\n",
      "Dropping columns / rows...\n",
      "Encoding data...\n",
      "\tUpdated column definition:\n",
      "\t\tid: REAL_VALUED (ID)\n",
      "\t\ttime: DATE (TIME)\n",
      "\t\tgl: REAL_VALUED (TARGET)\n",
      "\t\ttime_year: REAL_VALUED (KNOWN_INPUT)\n",
      "\t\ttime_month: REAL_VALUED (KNOWN_INPUT)\n",
      "\t\ttime_day: REAL_VALUED (KNOWN_INPUT)\n",
      "\t\ttime_hour: REAL_VALUED (KNOWN_INPUT)\n",
      "\t\ttime_minute: REAL_VALUED (KNOWN_INPUT)\n",
      "Interpolating data...\n",
      "\tDropped segments: 1416\n",
      "\tExtracted segments: 681\n",
      "\tInterpolated values: 140564\n",
      "\tPercent of values interpolated: 24.24%\n",
      "Splitting data...\n",
      "\tTrain: 357814 (68.58%)\n",
      "\tVal: 96960 (18.58%)\n",
      "\tTest: 125008 (23.96%)\n",
      "Scaling data...\n",
      "\tNo scaling applied\n",
      "Data formatting complete.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "def load_data(seed = 0, study_file = None):\n",
    "    # load data\n",
    "    with open('./config/weinstock.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    config['split_params']['random_state'] = seed\n",
    "    formatter = DataFormatter(config, study_file = study_file)\n",
    "\n",
    "    # convert to series\n",
    "    time_col = formatter.get_column('time')\n",
    "    group_col = formatter.get_column('sid')\n",
    "    target_col = formatter.get_column('target')\n",
    "    static_cols = formatter.get_column('static_covs')\n",
    "    static_cols = static_cols + [formatter.get_column('id')] if static_cols is not None else [formatter.get_column('id')]\n",
    "    dynamic_cols = formatter.get_column('dynamic_covs')\n",
    "    future_cols = formatter.get_column('future_covs')\n",
    "\n",
    "    # build series\n",
    "    series, scalers = make_series({'train': formatter.train_data,\n",
    "                                    'val': formatter.val_data,\n",
    "                                    'test': formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)],\n",
    "                                    'test_ood': formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]},\n",
    "                                    time_col,\n",
    "                                    group_col,\n",
    "                                    {'target': target_col,\n",
    "                                    'static': static_cols,\n",
    "                                    'dynamic': dynamic_cols,\n",
    "                                    'future': future_cols})\n",
    "    \n",
    "    return formatter, series, scalers\n",
    "\n",
    "formatter, series, scalers = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in series['train']['target']:\n",
    "    f.plot(label='train', alpha=0.1, color='grey')\n",
    "series['test_ood']['target'][0].plot(label='test_ood')\n",
    "for f in series['test']['target']:\n",
    "    f.plot(label='test_id', color='orange', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 18h segments that start at 6AM\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        serie = series['train']['target'][i+3*j]\n",
    "        start = None\n",
    "        for k, hour in enumerate(serie.time_index.hour):\n",
    "            if hour == 6:\n",
    "                start = k\n",
    "                break\n",
    "        if start is not None and start + 216 <= len(serie):\n",
    "            serie = serie[start:(start+216)]\n",
    "            serie.plot(ax=axs[i, j], label='Segment {}'.format(i+3*j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 18h segments that start at 6AM\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        serie = series['train']['target'][i+3*j]\n",
    "        start = None\n",
    "        for k, hour in enumerate(serie.time_index.hour):\n",
    "            if hour == 6:\n",
    "                start = k\n",
    "                break\n",
    "        if start is not None and start + 216 <= len(serie):\n",
    "            serie = serie[start:(start+216)]\n",
    "            # rescale\n",
    "            serie = scalers['target'].inverse_transform(serie)\n",
    "            # set y-axis limits for axs[i, j]\n",
    "            serie.plot(ax=axs[i, j], label='Segment {}'.format(i+3*j))\n",
    "            axs[i, j].set_ylim(0, 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_data, scaler, in_len, out_len, stride, target_col, group_col):\n",
    "    errors = []\n",
    "    for group, data in test_data.groupby(group_col):\n",
    "        train_set = data[target_col].iloc[:in_len].values.flatten()\n",
    "        # fit model\n",
    "        model = AutoARIMA(start_p = 0,\n",
    "                        max_p = 10,\n",
    "                        start_q = 0,\n",
    "                        max_q = 10,\n",
    "                        start_P = 0,\n",
    "                        max_P = 10,\n",
    "                        start_Q=0,\n",
    "                        max_Q=10,\n",
    "                        allowdrift=True,\n",
    "                        allowmean=True,\n",
    "                        parallel=False)\n",
    "        model.fit(train_set)\n",
    "        # get valid sampling locations for future prediction\n",
    "        start_idx = np.arange(start=stride, stop=len(data) - in_len - out_len + 1, step=stride)\n",
    "        end_idx = start_idx + in_len\n",
    "        # iterate and collect predictions\n",
    "        for i in range(len(start_idx)):\n",
    "            input = data[target_col].iloc[start_idx[i]:end_idx[i]].values.flatten()\n",
    "            true = data[target_col].iloc[end_idx[i]:(end_idx[i]+out_len)].values.flatten()\n",
    "            prediction = model.forward(input, h=out_len)['mean']\n",
    "            # unscale true and prediction\n",
    "            true = scaler.inverse_transform(true.reshape(-1, 1)).flatten()\n",
    "            prediction = scaler.inverse_transform(prediction.reshape(-1, 1)).flatten()\n",
    "            # collect errors\n",
    "            errors.append(np.array([np.mean((true - prediction)**2), np.mean(np.abs(true - prediction))]))\n",
    "    errors = np.vstack(errors)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('./config/iglu.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "config['split_params']['random_state'] = 0\n",
    "config['scaling_params']['scaler'] = 'StandardScaler'\n",
    "formatter = DataFormatter(config, study_file = None)\n",
    "\n",
    "# set params\n",
    "in_len = formatter.params['max_length_input']\n",
    "out_len = formatter.params['length_pred']\n",
    "stride = formatter.params['length_pred'] // 2\n",
    "target_col = formatter.get_column('target')\n",
    "group_col = formatter.get_column('sid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)]\n",
    "test_data_ood = formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]\n",
    "\n",
    "# backtest on the ID test set\n",
    "id_errors_sample = test_model(test_data, formatter.scalers[target_col[0]], in_len, out_len, stride, target_col, group_col)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "id_error_stats_sample = compute_error_statistics(id_errors_sample)\n",
    "\n",
    "# backtest on the ood test set\n",
    "ood_errors_sample = test_model(test_data_ood, formatter.scalers[target_col[0]], in_len, out_len, stride, target_col, group_col)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "ood_errors_stats_sample = compute_error_statistics(ood_errors_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LinearRegressionModel(lags = 32,\n",
    "                                     output_chunk_length = formatter.params['length_pred'])\n",
    "\n",
    "model.fit(series['train']['target'],\n",
    "          max_samples_per_ts=None, \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
    "                                    forecasts,  \n",
    "                                    [metrics.mse, metrics.mae],\n",
    "                                    scalers['target'],\n",
    "                                    reduction=None)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "np.median(id_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test_ood']['target'],\n",
    "                                                forecast_horizon=formatter.params['length_pred'], \n",
    "                                                stride=formatter.params['length_pred'] // 2,\n",
    "                                                retrain=False,\n",
    "                                                verbose=True,\n",
    "                                                last_points_only=False,\n",
    "                                                start=formatter.params[\"max_length_input\"])\n",
    "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
    "                            forecasts,  \n",
    "                            [metrics.mse, metrics.mae],\n",
    "                            scalers['target'],\n",
    "                            reduction=None)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "np.median(ood_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ts = scalers['target'].inverse_transform(series['test_ood']['target'][0])\n",
    "predicted_ts = [scalers['target'].inverse_transform(f) for f in forecasts]\n",
    "\n",
    "errors = [\n",
    "    [metric_f(target_ts, f) for metric_f in [metrics.mse, metrics.mae]]\n",
    "    for f in predicted_ts\n",
    "    if f.time_index.hour[0] > 6\n",
    "]\n",
    "\n",
    "errors = np.array(errors)\n",
    "errors.shape\n",
    "\n",
    "np.median(errors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['val']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:10:45] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"model_seed\" } are not used.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<darts.models.forecasting.xgboost.XGBModel at 0x7f2d1cc82a70>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.XGBModel(lags=96, \n",
    "                        learning_rate=0.773,\n",
    "                        subsample=0.8,\n",
    "                        min_child_weight=1.0,\n",
    "                        colsample_bytree=1.0,\n",
    "                        max_depth=6,\n",
    "                        gamma=0.5,\n",
    "                        reg_alpha=0.167,\n",
    "                        reg_lambda=0.229,\n",
    "                        n_estimators=352,\n",
    "                        model_seed=0)\n",
    "\n",
    "model.fit(series['train']['target'],\n",
    "          max_samples_per_ts=None, \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd0340b6c574abd8ce91a901dcb02f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([843.0783,  25.5631], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
    "                                    forecasts,  \n",
    "                                    [metrics.mse, metrics.mae],\n",
    "                                    scalers['target'],\n",
    "                                    reduction=None)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "np.median(id_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1846., 1836.,    0., 1861., 1900.,    0., 1951., 1969.,    0.,\n",
       "        2019., 2073.,    0., 2049.,    0., 1912., 1797.,    0., 1810.,\n",
       "        1772.,    0., 1739., 1750.,    0., 1777.,    0., 1790., 1788.,\n",
       "           0., 1802., 1812.,    0., 1815., 1840.,    0., 1856., 1844.]),\n",
       " array([ 0.        ,  0.63888889,  1.27777778,  1.91666667,  2.55555556,\n",
       "         3.19444444,  3.83333333,  4.47222222,  5.11111111,  5.75      ,\n",
       "         6.38888889,  7.02777778,  7.66666667,  8.30555556,  8.94444444,\n",
       "         9.58333333, 10.22222222, 10.86111111, 11.5       , 12.13888889,\n",
       "        12.77777778, 13.41666667, 14.05555556, 14.69444444, 15.33333333,\n",
       "        15.97222222, 16.61111111, 17.25      , 17.88888889, 18.52777778,\n",
       "        19.16666667, 19.80555556, 20.44444444, 21.08333333, 21.72222222,\n",
       "        22.36111111, 23.        ]),\n",
       " <BarContainer object of 36 artists>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "hours = []\n",
    "for i in range(len(series['test']['target'])):\n",
    "    hours += list(series['test']['target'][i].time_index[formatter.params['max_length_input']: ].hour)\n",
    "axs[0].hist(hours, bins=36)\n",
    "\n",
    "hours = []\n",
    "for i in range(len(series['test_ood']['target'])):\n",
    "    hours += list(series['test_ood']['target'][i].time_index[formatter.params['max_length_input']: ].hour)\n",
    "axs[1].hist(hours, bins=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a9ec9975f64d08ad66a8a3b0dd6195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([403.53268 ,  18.724794], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts = model.historical_forecasts(series['test_ood']['target'],\n",
    "                                                forecast_horizon=formatter.params['length_pred'], \n",
    "                                                stride=formatter.params['length_pred'] // 2,\n",
    "                                                retrain=False,\n",
    "                                                verbose=True,\n",
    "                                                last_points_only=False,\n",
    "                                                start=formatter.params[\"max_length_input\"])\n",
    "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
    "                            forecasts,  \n",
    "                            [metrics.mse, metrics.mae],\n",
    "                            scalers['target'],\n",
    "                            reduction=None)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "np.median(ood_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 2)\n",
      "[412.1863    18.355669]\n"
     ]
    }
   ],
   "source": [
    "def rescale_and_backtest(series: Union[TimeSeries, Sequence[TimeSeries]],\n",
    "                         forecasts: Union[TimeSeries, Sequence[TimeSeries]], \n",
    "                         metric: Union[\n",
    "                                    Callable[[TimeSeries, TimeSeries], float],\n",
    "                                    List[Callable[[TimeSeries, TimeSeries], float]],\n",
    "                                ], \n",
    "                         scaler: Callable[[TimeSeries], TimeSeries] = None,\n",
    "                         reduction: Union[Callable[[np.ndarray], float], None] = np.mean,\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Backtest the forecasts on the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series\n",
    "        The target time series.\n",
    "    forecasts\n",
    "        The forecasts.\n",
    "    scaler\n",
    "        The scaler used to scale the series.\n",
    "    metric\n",
    "        The metric to use for backtesting.\n",
    "    reduction\n",
    "        The reduction to apply to the metric.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float or List[float] or List[List[float]]\n",
    "        The (sequence of) error score on a series, or list of list containing error scores for each\n",
    "        provided series and each sample.\n",
    "    \"\"\"\n",
    "    series = [series] if isinstance(series, TimeSeries) else series\n",
    "    if len(series) == 1:\n",
    "        forecasts = [forecasts]\n",
    "    if not isinstance(metric, list):\n",
    "        metric = [metric]\n",
    "\n",
    "    # reverse scaling, forecasts and true values, compute errors\n",
    "    backtest_list = []\n",
    "    for idx, target_ts in enumerate(series):\n",
    "        if scaler is not None:\n",
    "            target_ts = scaler.inverse_transform(target_ts)\n",
    "            predicted_ts = [scaler.inverse_transform(f) for f in forecasts[idx]]\n",
    "        errors = [\n",
    "            [metric_f(target_ts, f) for metric_f in metric]\n",
    "            if len(metric) > 1\n",
    "            else metric[0](target_ts, f)\n",
    "            for f in predicted_ts\n",
    "            if f.time_index.hour[0] > 6 and f.time_index.hour[0] < 18\n",
    "        ]\n",
    "        if reduction is None:\n",
    "            backtest_list.append(np.array(errors))\n",
    "        else:\n",
    "            backtest_list.append(reduction(np.array(errors), axis=0))\n",
    "    return backtest_list if len(backtest_list) > 1 else backtest_list[0]\n",
    "\n",
    "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
    "                            forecasts,  \n",
    "                            [metrics.mse, metrics.mae],\n",
    "                            scalers['target'],\n",
    "                            reduction=None)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "np.median(ood_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        true = scalers['target'].inverse_transform(series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36))\n",
    "        forecast = scalers['target'].inverse_transform(f)\n",
    "        forecast.plot(ax=axs[j, i], label='forecast')\n",
    "        true.plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        forecast = scalers['target'].inverse_transform(forecasts[i+6*j])\n",
    "        true = scalers['target'].inverse_transform(series['test_ood']['target'][0].slice_n_points_after(forecast.time_index[0] - pd.Timedelta(\"2h\"), 36))\n",
    "        forecast.plot(ax=axs[j, i], label='forecast')\n",
    "        true.plot(ax=axs[j, i], label='true')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "model_name = f'tensorboard_tft_weinstock'\n",
    "work_dir = './output'\n",
    "el_stopper = EarlyStopping(\n",
    "                            monitor=\"val_loss\",\n",
    "                            patience=20,\n",
    "                            min_delta=0.001,\n",
    "                            mode='min',\n",
    "                            )\n",
    "loss_logger = LossLogger()\n",
    "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
    "\n",
    "# build the TFTModel model\n",
    "model = models.TFTModel(input_chunk_length = 96, \n",
    "                        output_chunk_length = formatter.params['length_pred'], \n",
    "                        hidden_size = 64,\n",
    "                        lstm_layers = 1,\n",
    "                        num_attention_heads = 4,\n",
    "                        full_attention = True,\n",
    "                        dropout = 0.1,\n",
    "                        hidden_continuous_size = 16,\n",
    "                        add_relative_index = True,\n",
    "                        model_name = model_name,\n",
    "                        work_dir = work_dir,\n",
    "                        log_tensorboard = True,\n",
    "                        pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                        batch_size = 64,\n",
    "                        optimizer_kwargs = {'lr': 0.001},\n",
    "                        save_checkpoints = True,\n",
    "                        force_reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(series=series['train']['target'],\n",
    "              val_series=series['val']['target'],\n",
    "              max_samples_per_ts=200,\n",
    "              verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=False,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = rescale_and_backtest(series['test']['target'],\n",
    "                                      forecasts,  \n",
    "                                      [metrics.mse, metrics.mae],\n",
    "                                      scalers['target'],\n",
    "                                      reduction=None)\n",
    "errors = np.vstack(errors)         \n",
    "np.median(errors, axis=0)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NHiTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "model_name = f'tensorboard_nhits_weinstock'\n",
    "work_dir = './output'\n",
    "el_stopper = EarlyStopping(monitor=\"val_loss\", patience=20, min_delta=0.001, mode='min')\n",
    "loss_logger = LossLogger()\n",
    "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
    "\n",
    "# build the TFTModel model\n",
    "model = models.NHiTSModel(input_chunk_length=96, \n",
    "                            output_chunk_length=12, \n",
    "                            num_stacks=3, \n",
    "                            num_blocks=1, \n",
    "                            num_layers=2, \n",
    "                            layer_widths=512,  \n",
    "                            n_freq_downsample=None, \n",
    "                            dropout=0.05, \n",
    "                            activation='ReLU',\n",
    "                            log_tensorboard = True,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            batch_size = 64,\n",
    "                            optimizer_kwargs = {'lr': 0.001},\n",
    "                            save_checkpoints = True,\n",
    "                            model_name = model_name,\n",
    "                            work_dir = work_dir,\n",
    "                            force_reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(series=series['train']['target'],\n",
    "              val_series=series['val']['target'],\n",
    "              max_samples_per_ts=200,\n",
    "              verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(model_name, work_dir = work_dir)\n",
    "\n",
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=False,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "\n",
    "errors = rescale_and_backtest(series['test']['target'],\n",
    "                                      forecasts,  \n",
    "                                      [metrics.mse, metrics.mae],\n",
    "                                      scalers['target'],\n",
    "                                      reduction=None)\n",
    "errors = np.vstack(errors)         \n",
    "np.median(errors, axis=0)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "model_name = f'tensorboard_transformer_weinstock'\n",
    "work_dir = './output'\n",
    "el_stopper = EarlyStopping(monitor=\"val_loss\", patience=20, min_delta=0.001, mode='min')\n",
    "loss_logger = LossLogger()\n",
    "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
    "\n",
    "# build the TFTModel model\n",
    "model = models.TransformerModel(input_chunk_length=96, \n",
    "                            output_chunk_length=12, \n",
    "                            d_model=64, \n",
    "                            nhead=4, \n",
    "                            num_encoder_layers=3, \n",
    "                            num_decoder_layers=3, \n",
    "                            dim_feedforward=512, \n",
    "                            dropout=0.1,\n",
    "                            log_tensorboard = True,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            batch_size = 64,\n",
    "                            optimizer_kwargs = {'lr': 0.001},\n",
    "                            save_checkpoints = True,\n",
    "                            model_name = model_name,\n",
    "                            work_dir = work_dir,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(series=series['train']['target'],\n",
    "              val_series=series['val']['target'],\n",
    "              max_samples_per_ts=200,\n",
    "              verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(model_name, work_dir = work_dir)\n",
    "\n",
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=False,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "\n",
    "errors = rescale_and_backtest(series['test']['target'],\n",
    "                                      forecasts,  \n",
    "                                      [metrics.mse, metrics.mae],\n",
    "                                      scalers['target'],\n",
    "                                      reduction=None)\n",
    "errors = np.vstack(errors)         \n",
    "np.median(errors, axis=0)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (with covariates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data and optional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed = 0, study_file = None):\n",
    "    # load data\n",
    "    with open('./config/weinstock.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    config['split_params']['random_state'] = seed\n",
    "    formatter = DataFormatter(config, study_file = study_file)\n",
    "\n",
    "    # convert to series\n",
    "    time_col = formatter.get_column('time')\n",
    "    group_col = formatter.get_column('sid')\n",
    "    target_col = formatter.get_column('target')\n",
    "    static_cols = formatter.get_column('static_covs')\n",
    "    static_cols = static_cols + [formatter.get_column('id')] if static_cols is not None else [formatter.get_column('id')]\n",
    "    dynamic_cols = formatter.get_column('dynamic_covs')\n",
    "    future_cols = formatter.get_column('future_covs')\n",
    "\n",
    "    # build series\n",
    "    series, scalers = make_series({'train': formatter.train_data,\n",
    "                                    'val': formatter.val_data,\n",
    "                                    'test': formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)],\n",
    "                                    'test_ood': formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]},\n",
    "                                    time_col,\n",
    "                                    group_col,\n",
    "                                    {'target': target_col,\n",
    "                                    'static': static_cols,\n",
    "                                    'dynamic': dynamic_cols,\n",
    "                                    'future': future_cols})\n",
    "    \n",
    "    return formatter, series, scalers\n",
    "\n",
    "formatter, series, scalers = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach static covariates to series\n",
    "for i in range(len(series['train']['target'])):\n",
    "    static_covs = series['train']['static'][i][0].pd_dataframe()\n",
    "    series['train']['target'][i] = series['train']['target'][i].with_static_covariates(static_covs)\n",
    "# attach to validation and test series\n",
    "for i in range(len(series['val']['target'])):\n",
    "    static_covs = series['val']['static'][i][0].pd_dataframe()\n",
    "    series['val']['target'][i] = series['val']['target'][i].with_static_covariates(static_covs)\n",
    "for i in range(len(series['test']['target'])):\n",
    "    static_covs = series['test']['static'][i][0].pd_dataframe()\n",
    "    series['test']['target'][i] = series['test']['target'][i].with_static_covariates(static_covs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LinearRegressionModel(lags = 60,\n",
    "                                     output_chunk_length = formatter.params['length_pred'])\n",
    "\n",
    "model.fit(series['train']['target'],\n",
    "          max_samples_per_ts=100, \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['val']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_sample = rescale_and_backtest(series['val']['target'],\n",
    "                                          forecasts,  \n",
    "                                          [metrics.mse, metrics.mae],\n",
    "                                          scalers['target'],\n",
    "                                          reduction=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(np.vstack(id_errors_sample), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['val']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9af0babfa4fcc32151d0f9cd96f26ee8eefb724c47cfe9b27c84c1db30f6822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

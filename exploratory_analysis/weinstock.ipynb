{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import warnings\n",
    "sys.path.insert(1, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "import optuna\n",
    "import darts\n",
    "\n",
    "from darts import models\n",
    "from darts import metrics\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "from statsforecast.models import AutoARIMA\n",
    "\n",
    "from data_formatter.base import *\n",
    "from bin.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing raw data and adding covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glucose data\n",
    "df = pd.read_csv('raw_data/Weinstock2016_allfiles/Data Tables/BDataCGM.txt', sep='|')\n",
    "# create Pandas start date and add days from DeviceDaysFromEnroll column\n",
    "for id, data in df.groupby('PtID'):\n",
    "    dates = pd.to_datetime('1900-01-01') + pd.to_timedelta(data['DeviceDaysFromEnroll'], unit='d')\n",
    "    df.loc[data.index, 'Date'] = dates\n",
    "# drop rows where glucose is NA\n",
    "df.dropna(inplace=True, subset='Glucose')\n",
    "# create full time column\n",
    "df['time'] =pd.to_datetime(df['Date'].astype(str) + ' ' + df['DeviceTm'])\n",
    "# rename Glucose column to gl and PtID to id\n",
    "df.rename(columns={'Glucose': 'gl', 'PtID': 'id'}, inplace=True)\n",
    "# drop all columns except id, time, and gl\n",
    "df.drop(columns=[col for col in df.columns if col not in ['gl', 'time', 'id']], inplace=True)\n",
    "# reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demographics data\n",
    "df_demo = pd.read_csv('raw_data/Weinstock2016_allfiles/Data Tables/BDemoLifeDiabHxMgmt.txt', sep='|')\n",
    "select_cols = ['PtID']\n",
    "# select gender\n",
    "select_cols.append('Gender')\n",
    "# select race\n",
    "select_cols.append('Race')\n",
    "# select Education Level\n",
    "select_cols.append('EduLevel')\n",
    "df_demo['EduLevel'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# convert to numeric based on the mapping \n",
    "# 'Unknown' = 0,\n",
    "# '7th or 8th Grade' = 1, \n",
    "# '9th Grade' = 2,\n",
    "# '11th Grade' = 3, \n",
    "# '12th Grade - no diploma' = 4,\n",
    "# 'High school graduate/diploma/GED' = 5,\n",
    "# # 'Some college but no degree' = 6, \n",
    "# 'Associate Degree' = 7,\n",
    "# # 'Professional Degree' = 8,  \n",
    "# 'Bachelor's Degree' = 9, \n",
    "# 'Master's Degree' = 10, \n",
    "# 'Doctorate Degree' = 11,\n",
    "df_demo['EduLevel'] = df_demo['EduLevel'].map({'Unknown': 0, \n",
    "                                               '7th or 8th Grade': 1, \n",
    "                                               '9th Grade': 2, \n",
    "                                               '11th Grade': 3, \n",
    "                                               '12th Grade - no diploma': 4, \n",
    "                                               'High school graduate/diploma/GED': 5, \n",
    "                                               'Some college but no degree': 6, \n",
    "                                               'Associate Degree': 7, \n",
    "                                               'Professional Degree': 8, \n",
    "                                               \"Bachelor's Degree\": 9, \n",
    "                                               \"Master's Degree\": 10, \n",
    "                                               \"Doctorate Degree\": 11})\n",
    "# select AnnualInc\n",
    "select_cols.append('AnnualInc')\n",
    "df_demo['AnnualInc'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# convert to numeric based on the mapping\n",
    "# 'Unknown' = 0,\n",
    "# 'Less than $25,000' = 1,\n",
    "# '$25,000 - $35,000' = 2, \n",
    "# '$35,000 - less than $50,000' = 3,\n",
    "# '$50,000 - less than $75,000' = 4,\n",
    "# '$75,000 - less than $100,000' = 5,\n",
    "# '$100,000 - less than $200,000' = 6\n",
    "# '$200,000 or more' = 7\n",
    "df_demo['AnnualInc'] = df_demo['AnnualInc'].map({'Unknown': 0,\n",
    "                                                 'Less than $25,000': 1,\n",
    "                                                 '$25,000 - $35,000': 2,\n",
    "                                                 '$35,000 - less than $50,000': 3,\n",
    "                                                 '$50,000 - less than $75,000': 4,\n",
    "                                                 '$75,000 - less than $100,000': 5,\n",
    "                                                 '$100,000 - less than $200,000': 6,\n",
    "                                                 '$200,000 or more': 7})\n",
    "\n",
    "# select MaritalStatus\n",
    "select_cols.append('MaritalStatus')\n",
    "df_demo['MaritalStatus'].fillna('Unknown', inplace=True) # replace NaN with 'Unknown'\n",
    "# select DaysWkEx\n",
    "select_cols.append('DaysWkEx')\n",
    "df_demo['DaysWkEx'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select DaysWkDrinkAlc\n",
    "select_cols.append('DaysWkDrinkAlc')\n",
    "df_demo['DaysWkDrinkAlc'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select DaysMonBingeAlc\n",
    "select_cols.append('DaysMonBingeAlc')\n",
    "df_demo['DaysMonBingeAlc'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select T1DDiagAge\n",
    "select_cols.append('T1DDiagAge')\n",
    "# select NumHospDKA\n",
    "select_cols.append('NumHospDKA')\n",
    "df_demo['NumHospDKA'].fillna(0, inplace=True) # replace NaN with 0\n",
    "# select NumSHSinceT1DDiag\n",
    "select_cols.append('NumSHSinceT1DDiag')\n",
    "# convert to numeric based on the mapping\n",
    "# '0' = 0, \n",
    "# '1' = 1\n",
    "# '2' = 2, \n",
    "# '3' = 3, \n",
    "# '4' = 4,\n",
    "# '5 - 9' = 5,\n",
    "# '10 - 19' = 6, \n",
    "# '>19' = 7\n",
    "df_demo['NumSHSinceT1DDiag'] = df_demo['NumSHSinceT1DDiag'].map({'0': 0,\n",
    "                                                                '1': 1,\n",
    "                                                                '2': 2,\n",
    "                                                                '3': 3,\n",
    "                                                                '4': 4,\n",
    "                                                                '5 - 9': 5,\n",
    "                                                                '10 - 19': 6,\n",
    "                                                                '>19': 7})\n",
    "# select InsDeliveryMethod\n",
    "select_cols.append('InsDeliveryMethod')\n",
    "# select UnitsInsTotal, replace NaN with 0\n",
    "select_cols.append('UnitsInsTotal')\n",
    "df_demo['UnitsInsTotal'].fillna(0, inplace=True)\n",
    "# select NumMeterCheckDay\n",
    "select_cols.append('NumMeterCheckDay')\n",
    "# convert to numeric based on the mapping\n",
    "# '0' = 0,\n",
    "# '1' = 1,\n",
    "# '2' = 2,\n",
    "# '3' = 3,\n",
    "# '4' = 4,\n",
    "# '5' = 5,\n",
    "# '6' = 6,\n",
    "# '7' = 7,\n",
    "# '8' = 8,\n",
    "# '9' = 9,\n",
    "# '10' = 10,\n",
    "# '11' = 11,\n",
    "# '12' = 12,\n",
    "# '13' = 13,\n",
    "# '14' = 14,\n",
    "# '15' = 15,\n",
    "# '16' = 16,\n",
    "# '17' = 17,\n",
    "# '18' = 18,\n",
    "# '> 19' = 19\n",
    "df_demo['NumMeterCheckDay'] = df_demo['NumMeterCheckDay'].map({'0': 0,\n",
    "                                                                '1': 1,\n",
    "                                                                '2': 2,\n",
    "                                                                '3': 3,\n",
    "                                                                '4': 4,\n",
    "                                                                '5': 5,\n",
    "                                                                '6': 6,\n",
    "                                                                '7': 7,\n",
    "                                                                '8': 8,\n",
    "                                                                '9': 9,\n",
    "                                                                '10': 10,\n",
    "                                                                '11': 11,\n",
    "                                                                '12': 12,\n",
    "                                                                '13': 13,\n",
    "                                                                '14': 14,\n",
    "                                                                '15': 15,\n",
    "                                                                '16': 16,\n",
    "                                                                '17': 17,\n",
    "                                                                '18': 18,\n",
    "                                                                '> 19': 19})\n",
    "# leave only selected columns\n",
    "df_demo = df_demo[select_cols]\n",
    "# rename PtID to id\n",
    "df_demo.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# print selected columns\n",
    "print(df_demo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medical conditions data\n",
    "df_medchart = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedChart.txt', sep='|')\n",
    "# convert weight to lbs\n",
    "df_medchart.loc[df_medchart['WeightUnits'] == 'kg', 'Weight'] = df_medchart.loc[df_medchart['WeightUnits'] == 'kg', 'Weight'] * 2.20462\n",
    "# convert height to inches\n",
    "df_medchart.loc[df_medchart['HeightUnits'] == 'cm', 'Height'] = df_medchart.loc[df_medchart['HeightUnits'] == 'cm', 'Height'] * 0.393701\n",
    "# select Height and Weight and fill NaN with 0\n",
    "df_medchart['Height'].fillna(0, inplace=True)\n",
    "df_medchart['Weight'].fillna(0, inplace=True)\n",
    "df_medchart = df_medchart[['PtID', 'Height', 'Weight']]\n",
    "# rename PtID to id\n",
    "df_medchart.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# print selected columns\n",
    "print(df_medchart.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medical conditions data\n",
    "df_medcond = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedicalConditions.txt', sep='|')\n",
    "# select top-13 illnesses (>10% of 201 patients have at least one of them based on value counts)\n",
    "top13_illnesses = df_medcond['MCLLTReal'].value_counts().index[:13]\n",
    "# create a one-hot encoding of the top-13 illnesses\n",
    "df_medcond = pd.get_dummies(df_medcond, columns=['MCLLTReal'], prefix='', prefix_sep='', dummy_na=True)\n",
    "df_medcond = df_medcond[['PtID'] + top13_illnesses.tolist()]\n",
    "# remove zero rows\n",
    "df_medcond = df_medcond.loc[(df_medcond[top13_illnesses] != 0).any(axis=1)]\n",
    "# rename PtID to id\n",
    "df_medcond.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# sum rows for the same id\n",
    "df_medcond = df_medcond.groupby('id').sum()\n",
    "# reset index\n",
    "df_medcond.reset_index(inplace=True)\n",
    "# print top-13 illnesses\n",
    "print(top13_illnesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load medication data\n",
    "df_med = pd.read_csv('./raw_data/Weinstock2016_allfiles/Data Tables/BMedication.txt', sep='|')\n",
    "# select top-9 medications (>10% of 201 patients have at least one of them based on value counts)\n",
    "all_meds = df_med['DrugName'].unique()\n",
    "top9_meds = df_med['DrugName'].value_counts().index[:9]\n",
    "# create a one-hot encoding of the top-9 medications\n",
    "df_med = pd.get_dummies(df_med, columns=['DrugName'], prefix='', prefix_sep='', dummy_na=True)\n",
    "\n",
    "# strip first number from MedDose\n",
    "import re\n",
    "def strip_first_number(x):\n",
    "    x = str(x)\n",
    "    # remove all ,\n",
    "    x = x.replace(',', '')\n",
    "    # find first non-number and not . or , character in string x\n",
    "    first_non_num = re.search(r'[^0-9.]', x)\n",
    "    if first_non_num is None:\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float(x[:first_non_num.start()]) if first_non_num.start() > 0 else 1.0\n",
    "# apply strip_first_number to MedDose per element\n",
    "df_med['MedDose'].fillna(1, inplace=True)\n",
    "for i in range(len(df_med)):\n",
    "    df_med['MedDose'].iloc[i] = strip_first_number(df_med['MedDose'].iloc[i])\n",
    "\n",
    "# for each patient get the dose for each medication\n",
    "df_med[all_meds].values[df_med[all_meds] != 0] = df_med['MedDose']\n",
    "# select PtID and top-9 medications\n",
    "df_med = df_med[['PtID'] + top9_meds.tolist()]\n",
    "# remove zero rows\n",
    "df_med = df_med.loc[(df_med[top9_meds] != 0).any(axis=1)]\n",
    "# rename PtID to id\n",
    "df_med.rename(columns={'PtID': 'id'}, inplace=True)\n",
    "# sum rows for the same id\n",
    "df_med = df_med.groupby('id').sum()\n",
    "# reset index\n",
    "df_med.reset_index(inplace=True)\n",
    "# print top-9 medications\n",
    "print(top9_meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dataframes\n",
    "df_new = df.merge(df_demo, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_medchart, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_medcond, on = 'id', how='left')\n",
    "df_new = df_new.merge(df_med, on = 'id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na values with zeros for df_med and df_medcond columns\n",
    "df_new[top13_illnesses] = df_new[top13_illnesses].fillna(0)\n",
    "df_new[top9_meds] = df_new[top9_meds].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as Weinstock2016_processed.csv\n",
    "df_new.to_csv('./raw_data/Weinstock2016_processed_with_covariates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml config file\n",
    "with open('./config/weinstock.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# set interpolation params for no interpolation\n",
    "new_config = config.copy()\n",
    "new_config['interpolation_params']['gap_threshold'] = 5\n",
    "new_config['interpolation_params']['min_drop_length'] = 0\n",
    "# set split params for no splitting\n",
    "new_config['split_params']['test_percent_subjects'] = 0\n",
    "new_config['split_params']['length_segment'] = 0\n",
    "# set scaling params for no scaling\n",
    "new_config['scaling_params']['scaler'] = 'None'\n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 9 segments\n",
    "num_segments = 9\n",
    "plot_data = formatter.train_data\n",
    "\n",
    "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))\n",
    "    if i >= num_segments - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of random samples from first 9 segments segments\n",
    "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
    "lags = 300; k = 0\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    else:\n",
    "        # select 10 random samples from index of data\n",
    "        sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "        # plot acf / pacf of each sample\n",
    "        for j in sample:\n",
    "            acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "            pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "            ax[0, k].plot(acf)\n",
    "            ax[1, k].plot(pacf)\n",
    "        k += 1\n",
    "        if k >= num_segments:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF plots suggest that significant dependency persists up to 200 points (~16 hours). The analysis of distribution of segment lengths suggests that there are too many short segments. \n",
    "Based on this, interpolation should be performed of missing values up to 45 minutes (9 points), segments less than 200 points should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set interpolation params for interpolation\n",
    "new_config['interpolation_params']['gap_threshold'] = 45 # minutes - use as in config file \n",
    "new_config['interpolation_params']['min_drop_length'] = 240\n",
    "\n",
    "formatter = DataFormatter(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print min, max, median, mean, std of segment lengths\n",
    "segment_lens = []\n",
    "for group, data in formatter.train_data.groupby('id_segment'):\n",
    "    segment_lens.append(len(data))\n",
    "print('Train segment lengths:')\n",
    "print('\\tMin: ', min(segment_lens))\n",
    "print('\\tMax: ', max(segment_lens))\n",
    "print('\\t1st Quartile: ', np.quantile(segment_lens, 0.25))\n",
    "print('\\tMedian: ', np.median(segment_lens))\n",
    "print('\\t3rd Quartile: ', np.quantile(segment_lens, 0.75))\n",
    "print('\\tMean: ', np.mean(segment_lens))\n",
    "print('\\tStd: ', np.std(segment_lens))\n",
    "\n",
    "# plot first 9 segments\n",
    "num_segments = 9\n",
    "plot_data = formatter.train_data\n",
    "\n",
    "fig, axs = plt.subplots(1, num_segments, figsize=(30, 5))\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data.plot(x='time', y='gl', ax=axs[i], title='Segment {}'.format(group))\n",
    "    if i >= num_segments - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acf of random samples from first 9 segments segments\n",
    "fig, ax = plt.subplots(2, num_segments, figsize=(30, 5))\n",
    "lags = 300; k = 0\n",
    "for i, (group, data) in enumerate(plot_data.groupby('id_segment')):\n",
    "    data = data['gl']\n",
    "    if len(data) < lags:\n",
    "        print('Segment {} is too short'.format(group))\n",
    "        continue\n",
    "    else:\n",
    "        # select 10 random samples from index of data\n",
    "        sample = np.random.choice(range(len(data))[:-lags], 10, replace=False)\n",
    "        # plot acf / pacf of each sample\n",
    "        for j in sample:\n",
    "            acf, acf_ci = sm.tsa.stattools.acf(data[j:j+lags], nlags=lags, alpha=0.05)\n",
    "            pacf, pacf_ci = sm.tsa.stattools.pacf(data[j:j+lags], method='ols-adjusted', alpha=0.05)\n",
    "            ax[0, k].plot(acf)\n",
    "            ax[1, k].plot(pacf)\n",
    "        k += 1\n",
    "        if k >= num_segments:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very hard to name the proper parameters for ARIMA model based on current ACF and PACF plots since within each segment, samples are behaving very differently showing different structures suitable for ARIMA model. However, we can still spot some common traits between segments. First, the autocorrelation graphs decays exponentially for almost every segment, on average, up to 20-50 lags (in some cases up to 100). Hence, the Auto Regression (AR) parameter can be set around these numbers. The partial autocorrelation plots pick around 2 for the first time and become close to zero after 5 lags at max. So, the Moving Average (MA) parameter can be set at 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the config according to the observations above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config/weinstock.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "# set interpolation params for no interpolation\n",
    "config['interpolation_params']['gap_threshold'] = 45\n",
    "config['interpolation_params']['min_drop_length'] = 240\n",
    "# set split params for no splitting\n",
    "config['split_params']['test_percent_subjects'] = 0.1\n",
    "config['split_params']['length_segment'] = 240\n",
    "# set scaling params for no scaling\n",
    "config['scaling_params']['scaler'] = 'None'\n",
    "\n",
    "formatter = DataFormatter(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models (no covariates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data and (optional) scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed = 0, study_file = None):\n",
    "    # load data\n",
    "    with open('./config/weinstock.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    config['split_params']['random_state'] = seed\n",
    "    formatter = DataFormatter(config, study_file = study_file)\n",
    "\n",
    "    # convert to series\n",
    "    time_col = formatter.get_column('time')\n",
    "    group_col = formatter.get_column('sid')\n",
    "    target_col = formatter.get_column('target')\n",
    "    static_cols = formatter.get_column('static_covs')\n",
    "    static_cols = static_cols + [formatter.get_column('id')] if static_cols is not None else [formatter.get_column('id')]\n",
    "    dynamic_cols = formatter.get_column('dynamic_covs')\n",
    "    future_cols = formatter.get_column('future_covs')\n",
    "\n",
    "    # build series\n",
    "    series, scalers = make_series({'train': formatter.train_data,\n",
    "                                    'val': formatter.val_data,\n",
    "                                    'test': formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)],\n",
    "                                    'test_ood': formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]},\n",
    "                                    time_col,\n",
    "                                    group_col,\n",
    "                                    {'target': target_col,\n",
    "                                    'static': static_cols,\n",
    "                                    'dynamic': dynamic_cols,\n",
    "                                    'future': future_cols})\n",
    "    \n",
    "    return formatter, series, scalers\n",
    "\n",
    "formatter, series, scalers = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "hours = []\n",
    "for i in range(len(series['test']['target'])):\n",
    "    hours += list(series['test']['target'][i].time_index[formatter.params['max_length_input']: ].hour)\n",
    "axs[0].hist(hours, bins=36)\n",
    "\n",
    "hours = []\n",
    "for i in range(len(series['test_ood']['target'])):\n",
    "    hours += list(series['test_ood']['target'][i].time_index[formatter.params['max_length_input']: ].hour)\n",
    "axs[1].hist(hours, bins=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in series['train']['target']:\n",
    "    f.plot(label='train', alpha=0.1, color='grey')\n",
    "series['test_ood']['target'][0].plot(label='test_ood')\n",
    "for f in series['test']['target']:\n",
    "    f.plot(label='test_id', color='orange', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 18h segments that start at 6AM\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        serie = series['train']['target'][i+3*j]\n",
    "        start = None\n",
    "        for k, hour in enumerate(serie.time_index.hour):\n",
    "            if hour == 6:\n",
    "                start = k\n",
    "                break\n",
    "        if start is not None and start + 216 <= len(serie):\n",
    "            serie = serie[start:(start+216)]\n",
    "            serie.plot(ax=axs[i, j], label='Segment {}'.format(i+3*j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 18h segments that start at 6AM\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        serie = series['train']['target'][i+3*j]\n",
    "        start = None\n",
    "        for k, hour in enumerate(serie.time_index.hour):\n",
    "            if hour == 6:\n",
    "                start = k\n",
    "                break\n",
    "        if start is not None and start + 216 <= len(serie):\n",
    "            serie = serie[start:(start+216)]\n",
    "            # rescale\n",
    "            serie = scalers['target'].inverse_transform(serie)\n",
    "            # set y-axis limits for axs[i, j]\n",
    "            serie.plot(ax=axs[i, j], label='Segment {}'.format(i+3*j))\n",
    "            axs[i, j].set_ylim(0, 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_data, scaler, in_len, out_len, stride, target_col, group_col):\n",
    "    errors = []\n",
    "    for group, data in test_data.groupby(group_col):\n",
    "        train_set = data[target_col].iloc[:in_len].values.flatten()\n",
    "        # fit model\n",
    "        model = AutoARIMA(start_p = 0,\n",
    "                        max_p = 10,\n",
    "                        start_q = 0,\n",
    "                        max_q = 10,\n",
    "                        start_P = 0,\n",
    "                        max_P = 10,\n",
    "                        start_Q=0,\n",
    "                        max_Q=10,\n",
    "                        allowdrift=True,\n",
    "                        allowmean=True,\n",
    "                        parallel=False)\n",
    "        model.fit(train_set)\n",
    "        # get valid sampling locations for future prediction\n",
    "        start_idx = np.arange(start=stride, stop=len(data) - in_len - out_len + 1, step=stride)\n",
    "        end_idx = start_idx + in_len\n",
    "        # iterate and collect predictions\n",
    "        for i in range(len(start_idx)):\n",
    "            input = data[target_col].iloc[start_idx[i]:end_idx[i]].values.flatten()\n",
    "            true = data[target_col].iloc[end_idx[i]:(end_idx[i]+out_len)].values.flatten()\n",
    "            prediction = model.forward(input, h=out_len)['mean']\n",
    "            # unscale true and prediction\n",
    "            true = scaler.inverse_transform(true.reshape(-1, 1)).flatten()\n",
    "            prediction = scaler.inverse_transform(prediction.reshape(-1, 1)).flatten()\n",
    "            # collect errors\n",
    "            errors.append(np.array([np.mean((true - prediction)**2), np.mean(np.abs(true - prediction))]))\n",
    "    errors = np.vstack(errors)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('./config/iglu.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "config['split_params']['random_state'] = 0\n",
    "config['scaling_params']['scaler'] = 'StandardScaler'\n",
    "formatter = DataFormatter(config, study_file = None)\n",
    "\n",
    "# set params\n",
    "in_len = formatter.params['max_length_input']\n",
    "out_len = formatter.params['length_pred']\n",
    "stride = formatter.params['length_pred'] // 2\n",
    "target_col = formatter.get_column('target')\n",
    "group_col = formatter.get_column('sid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)]\n",
    "test_data_ood = formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]\n",
    "\n",
    "# backtest on the ID test set\n",
    "id_errors_sample = test_model(test_data, formatter.scalers[target_col[0]], in_len, out_len, stride, target_col, group_col)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "id_error_stats_sample = compute_error_statistics(id_errors_sample)\n",
    "\n",
    "# backtest on the ood test set\n",
    "ood_errors_sample = test_model(test_data_ood, formatter.scalers[target_col[0]], in_len, out_len, stride, target_col, group_col)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "ood_errors_stats_sample = compute_error_statistics(ood_errors_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LinearRegressionModel(lags = 60,\n",
    "                                     output_chunk_length = formatter.params['length_pred'])\n",
    "\n",
    "model.fit(series['train']['target'],\n",
    "          max_samples_per_ts=None, \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
    "                                    forecasts,  \n",
    "                                    [metrics.mse, metrics.mae],\n",
    "                                    scalers['target'],\n",
    "                                    reduction=None)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "np.median(id_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test_ood']['target'],\n",
    "                                                forecast_horizon=formatter.params['length_pred'], \n",
    "                                                stride=formatter.params['length_pred'] // 2,\n",
    "                                                retrain=False,\n",
    "                                                verbose=True,\n",
    "                                                last_points_only=False,\n",
    "                                                start=formatter.params[\"max_length_input\"])\n",
    "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
    "                            forecasts,  \n",
    "                            [metrics.mse, metrics.mae],\n",
    "                            scalers['target'],\n",
    "                            reduction=None)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "np.median(ood_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['val']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.XGBModel(lags=96, \n",
    "                        learning_rate=0.773,\n",
    "                        subsample=0.8,\n",
    "                        min_child_weight=1.0,\n",
    "                        colsample_bytree=1.0,\n",
    "                        max_depth=6,\n",
    "                        gamma=0.5,\n",
    "                        reg_alpha=0.167,\n",
    "                        reg_lambda=0.229,\n",
    "                        n_estimators=352,\n",
    "                        model_seed=0)\n",
    "\n",
    "model.fit(series['train']['target'],\n",
    "          max_samples_per_ts=None, \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
    "                                    forecasts,  \n",
    "                                    [metrics.mse, metrics.mae],\n",
    "                                    scalers['target'],\n",
    "                                    reduction=None)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "np.median(id_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test_ood']['target'],\n",
    "                                                forecast_horizon=formatter.params['length_pred'], \n",
    "                                                stride=formatter.params['length_pred'] // 2,\n",
    "                                                retrain=False,\n",
    "                                                verbose=True,\n",
    "                                                last_points_only=False,\n",
    "                                                start=formatter.params[\"max_length_input\"])\n",
    "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
    "                            forecasts,  \n",
    "                            [metrics.mse, metrics.mae],\n",
    "                            scalers['target'],\n",
    "                            reduction=None)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "np.median(ood_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_and_backtest(series: Union[TimeSeries, Sequence[TimeSeries]],\n",
    "                         forecasts: Union[TimeSeries, Sequence[TimeSeries]], \n",
    "                         metric: Union[\n",
    "                                    Callable[[TimeSeries, TimeSeries], float],\n",
    "                                    List[Callable[[TimeSeries, TimeSeries], float]],\n",
    "                                ], \n",
    "                         scaler: Callable[[TimeSeries], TimeSeries] = None,\n",
    "                         reduction: Union[Callable[[np.ndarray], float], None] = np.mean,\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Backtest the forecasts on the series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series\n",
    "        The target time series.\n",
    "    forecasts\n",
    "        The forecasts.\n",
    "    scaler\n",
    "        The scaler used to scale the series.\n",
    "    metric\n",
    "        The metric to use for backtesting.\n",
    "    reduction\n",
    "        The reduction to apply to the metric.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float or List[float] or List[List[float]]\n",
    "        The (sequence of) error score on a series, or list of list containing error scores for each\n",
    "        provided series and each sample.\n",
    "    \"\"\"\n",
    "    series = [series] if isinstance(series, TimeSeries) else series\n",
    "    if len(series) == 1:\n",
    "        forecasts = [forecasts]\n",
    "    if not isinstance(metric, list):\n",
    "        metric = [metric]\n",
    "\n",
    "    # reverse scaling, forecasts and true values, compute errors\n",
    "    backtest_list = []\n",
    "    for idx, target_ts in enumerate(series):\n",
    "        if scaler is not None:\n",
    "            target_ts = scaler.inverse_transform(target_ts)\n",
    "            predicted_ts = [scaler.inverse_transform(f) for f in forecasts[idx]]\n",
    "        errors = [\n",
    "            [metric_f(target_ts, f) for metric_f in metric]\n",
    "            if len(metric) > 1\n",
    "            else metric[0](target_ts, f)\n",
    "            for f in predicted_ts\n",
    "            if f.time_index.hour[0] > 6 and f.time_index.hour[0] < 18\n",
    "        ]\n",
    "        if reduction is None:\n",
    "            backtest_list.append(np.array(errors))\n",
    "        else:\n",
    "            backtest_list.append(reduction(np.array(errors), axis=0))\n",
    "    return backtest_list if len(backtest_list) > 1 else backtest_list[0]\n",
    "\n",
    "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
    "                            forecasts,  \n",
    "                            [metrics.mse, metrics.mae],\n",
    "                            scalers['target'],\n",
    "                            reduction=None)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "np.median(ood_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        true = scalers['target'].inverse_transform(series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36))\n",
    "        forecast = scalers['target'].inverse_transform(f)\n",
    "        forecast.plot(ax=axs[j, i], label='forecast')\n",
    "        true.plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        forecast = scalers['target'].inverse_transform(forecasts[i+6*j])\n",
    "        true = scalers['target'].inverse_transform(series['test_ood']['target'][0].slice_n_points_after(forecast.time_index[0] - pd.Timedelta(\"2h\"), 36))\n",
    "        forecast.plot(ax=axs[j, i], label='forecast')\n",
    "        true.plot(ax=axs[j, i], label='true')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "model_name = f'tensorboard_tft_weinstock'\n",
    "work_dir = './output'\n",
    "el_stopper = EarlyStopping(\n",
    "                            monitor=\"val_loss\",\n",
    "                            patience=20,\n",
    "                            min_delta=0.001,\n",
    "                            mode='min',\n",
    "                            )\n",
    "loss_logger = LossLogger()\n",
    "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
    "\n",
    "# build the TFTModel model\n",
    "model = models.TFTModel(input_chunk_length = 96, \n",
    "                        output_chunk_length = formatter.params['length_pred'], \n",
    "                        hidden_size = 64,\n",
    "                        lstm_layers = 1,\n",
    "                        num_attention_heads = 4,\n",
    "                        full_attention = True,\n",
    "                        dropout = 0.1,\n",
    "                        hidden_continuous_size = 16,\n",
    "                        add_relative_index = True,\n",
    "                        model_name = model_name,\n",
    "                        work_dir = work_dir,\n",
    "                        log_tensorboard = True,\n",
    "                        pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                        batch_size = 64,\n",
    "                        optimizer_kwargs = {'lr': 0.001},\n",
    "                        save_checkpoints = True,\n",
    "                        force_reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(series=series['train']['target'],\n",
    "              val_series=series['val']['target'],\n",
    "              max_samples_per_ts=200,\n",
    "              verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=False,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = rescale_and_backtest(series['test']['target'],\n",
    "                                      forecasts,  \n",
    "                                      [metrics.mse, metrics.mae],\n",
    "                                      scalers['target'],\n",
    "                                      reduction=None)\n",
    "errors = np.vstack(errors)         \n",
    "np.median(errors, axis=0)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NHiTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "model_name = f'tensorboard_nhits_weinstock'\n",
    "work_dir = './output'\n",
    "el_stopper = EarlyStopping(monitor=\"val_loss\", patience=20, min_delta=0.001, mode='min')\n",
    "loss_logger = LossLogger()\n",
    "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
    "\n",
    "# build the TFTModel model\n",
    "model = models.NHiTSModel(input_chunk_length=96, \n",
    "                            output_chunk_length=12, \n",
    "                            num_stacks=3, \n",
    "                            num_blocks=1, \n",
    "                            num_layers=2, \n",
    "                            layer_widths=512,  \n",
    "                            n_freq_downsample=None, \n",
    "                            dropout=0.05, \n",
    "                            activation='ReLU',\n",
    "                            log_tensorboard = True,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            batch_size = 64,\n",
    "                            optimizer_kwargs = {'lr': 0.001},\n",
    "                            save_checkpoints = True,\n",
    "                            model_name = model_name,\n",
    "                            work_dir = work_dir,\n",
    "                            force_reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(series=series['train']['target'],\n",
    "              val_series=series['val']['target'],\n",
    "              max_samples_per_ts=200,\n",
    "              verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(model_name, work_dir = work_dir)\n",
    "\n",
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=False,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "\n",
    "errors = rescale_and_backtest(series['test']['target'],\n",
    "                                      forecasts,  \n",
    "                                      [metrics.mse, metrics.mae],\n",
    "                                      scalers['target'],\n",
    "                                      reduction=None)\n",
    "errors = np.vstack(errors)         \n",
    "np.median(errors, axis=0)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "model_name = f'tensorboard_transformer_weinstock'\n",
    "work_dir = './output'\n",
    "el_stopper = EarlyStopping(monitor=\"val_loss\", patience=20, min_delta=0.001, mode='min')\n",
    "loss_logger = LossLogger()\n",
    "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
    "\n",
    "# build the TFTModel model\n",
    "model = models.TransformerModel(input_chunk_length=96, \n",
    "                            output_chunk_length=12, \n",
    "                            d_model=64, \n",
    "                            nhead=4, \n",
    "                            num_encoder_layers=3, \n",
    "                            num_decoder_layers=3, \n",
    "                            dim_feedforward=512, \n",
    "                            dropout=0.1,\n",
    "                            log_tensorboard = True,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            batch_size = 64,\n",
    "                            optimizer_kwargs = {'lr': 0.001},\n",
    "                            save_checkpoints = True,\n",
    "                            model_name = model_name,\n",
    "                            work_dir = work_dir,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(series=series['train']['target'],\n",
    "              val_series=series['val']['target'],\n",
    "              max_samples_per_ts=200,\n",
    "              verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(model_name, work_dir = work_dir)\n",
    "\n",
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=False,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "\n",
    "errors = rescale_and_backtest(series['test']['target'],\n",
    "                                      forecasts,  \n",
    "                                      [metrics.mse, metrics.mae],\n",
    "                                      scalers['target'],\n",
    "                                      reduction=None)\n",
    "errors = np.vstack(errors)         \n",
    "np.median(errors, axis=0)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['test']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (with covariates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data and optional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed = 0, study_file = None):\n",
    "    # load data\n",
    "    with open('./config/weinstock.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    config['split_params']['random_state'] = seed\n",
    "    formatter = DataFormatter(config, study_file = study_file)\n",
    "\n",
    "    # convert to series\n",
    "    time_col = formatter.get_column('time')\n",
    "    group_col = formatter.get_column('sid')\n",
    "    target_col = formatter.get_column('target')\n",
    "    static_cols = formatter.get_column('static_covs')\n",
    "    static_cols = static_cols + [formatter.get_column('id')] if static_cols is not None else [formatter.get_column('id')]\n",
    "    dynamic_cols = formatter.get_column('dynamic_covs')\n",
    "    future_cols = formatter.get_column('future_covs')\n",
    "\n",
    "    # build series\n",
    "    series, scalers = make_series({'train': formatter.train_data,\n",
    "                                    'val': formatter.val_data,\n",
    "                                    'test': formatter.test_data.loc[~formatter.test_data.index.isin(formatter.test_idx_ood)],\n",
    "                                    'test_ood': formatter.test_data.loc[formatter.test_data.index.isin(formatter.test_idx_ood)]},\n",
    "                                    time_col,\n",
    "                                    group_col,\n",
    "                                    {'target': target_col,\n",
    "                                    'static': static_cols,\n",
    "                                    'dynamic': dynamic_cols,\n",
    "                                    'future': future_cols})\n",
    "    \n",
    "    # attach static covariates to series\n",
    "    for i in range(len(series['train']['target'])):\n",
    "        static_covs = series['train']['static'][i][0].pd_dataframe()\n",
    "        series['train']['target'][i] = series['train']['target'][i].with_static_covariates(static_covs)\n",
    "    # attach to validation and test series\n",
    "    for i in range(len(series['val']['target'])):\n",
    "        static_covs = series['val']['static'][i][0].pd_dataframe()\n",
    "        series['val']['target'][i] = series['val']['target'][i].with_static_covariates(static_covs)\n",
    "    for i in range(len(series['test']['target'])):\n",
    "        static_covs = series['test']['static'][i][0].pd_dataframe()\n",
    "        series['test']['target'][i] = series['test']['target'][i].with_static_covariates(static_covs)\n",
    "    \n",
    "    return formatter, series, scalers\n",
    "\n",
    "formatter, series, scalers = load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(series['train']['target'])):\n",
    "    assert len(series['train']['future'][i]) == len(series['train']['target'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LinearRegressionModel(lags = 192,\n",
    "                                     lags_future_covariates = (192, 12),\n",
    "                                     output_chunk_length = formatter.params['length_pred'])\n",
    "\n",
    "model.fit(series['train']['target'],\n",
    "          future_covariates=series['train']['future'],\n",
    "          max_samples_per_ts=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        future_covariates=series['test']['future'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
    "                                    forecasts,  \n",
    "                                    [metrics.mse, metrics.mae],\n",
    "                                    scalers['target'],\n",
    "                                    reduction=None)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "np.median(id_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test_ood']['target'],\n",
    "                                        future_covariates=series['test_ood']['future'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params[\"max_length_input\"])\n",
    "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
    "                            forecasts,  \n",
    "                            [metrics.mse, metrics.mae],\n",
    "                            scalers['target'],\n",
    "                            reduction=None)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "np.median(ood_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 6, figsize=(50, 20))\n",
    "for i in range(6):\n",
    "    for j, f in enumerate(forecasts[i][:5]):\n",
    "        f.plot(ax=axs[j, i], label='forecast')\n",
    "        series['val']['target'][i].slice_n_points_after(f.time_index[0] - pd.Timedelta(\"2h\"), 36).plot(ax=axs[j, i], label='true')\n",
    "        axs[j, i].legend(fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.XGBModel(lags=96, \n",
    "                        lags_future_covariates=(96, 12),\n",
    "                        learning_rate=0.773,\n",
    "                        subsample=0.8,\n",
    "                        min_child_weight=1.0,\n",
    "                        colsample_bytree=1.0,\n",
    "                        max_depth=6,\n",
    "                        gamma=0.5,\n",
    "                        reg_alpha=0.167,\n",
    "                        reg_lambda=0.229,\n",
    "                        n_estimators=352,\n",
    "                        model_seed=0)\n",
    "\n",
    "model.fit(series['train']['target'],\n",
    "          future_covariates=series['train']['future'],\n",
    "          max_samples_per_ts=None, \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        future_covariates=series['test']['future'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
    "                                        forecasts,  \n",
    "                                        [metrics.mse, metrics.mae],\n",
    "                                        scalers['target'],\n",
    "                                        reduction=None)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "np.median(id_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = model.historical_forecasts(series['test_ood']['target'],\n",
    "                                        future_covariates=series['test_ood']['future'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params[\"max_length_input\"])\n",
    "ood_errors_sample = rescale_and_backtest(series['test_ood']['target'],\n",
    "                            forecasts,  \n",
    "                            [metrics.mse, metrics.mae],\n",
    "                            scalers['target'],\n",
    "                            reduction=None)\n",
    "ood_errors_sample = np.vstack(ood_errors_sample)\n",
    "np.median(ood_errors_sample, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "model_name = f'tensorboard_tft_covariates_weinstock'\n",
    "work_dir = './output'\n",
    "el_stopper = EarlyStopping(\n",
    "                            monitor=\"val_loss\",\n",
    "                            patience=20,\n",
    "                            min_delta=0.001,\n",
    "                            mode='min',\n",
    "                            )\n",
    "loss_logger = LossLogger()\n",
    "pl_trainer_kwargs = {\"accelerator\": \"gpu\", \"devices\": [0], \"callbacks\": [el_stopper, loss_logger]}\n",
    "\n",
    "# build the TFTModel model\n",
    "model = models.TFTModel(input_chunk_length = 96, \n",
    "                        output_chunk_length = formatter.params['length_pred'], \n",
    "                        hidden_size = 64,\n",
    "                        lstm_layers = 1,\n",
    "                        num_attention_heads = 4,\n",
    "                        full_attention = True,\n",
    "                        dropout = 0.1,\n",
    "                        hidden_continuous_size = 16,\n",
    "                        add_relative_index = True,\n",
    "                        model_name = model_name,\n",
    "                        work_dir = work_dir,\n",
    "                        log_tensorboard = True,\n",
    "                        pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                        batch_size = 64,\n",
    "                        optimizer_kwargs = {'lr': 0.001},\n",
    "                        save_checkpoints = True,\n",
    "                        force_reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(series=series['train']['target'],\n",
    "          future_covariates=series['train']['future'],\n",
    "          val_series=series['val']['target'],\n",
    "          val_future_covariates=series['val']['future'],\n",
    "          max_samples_per_ts=200,\n",
    "          verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(model_name, work_dir = work_dir)\n",
    "\n",
    "forecasts = model.historical_forecasts(series['test']['target'],\n",
    "                                        future_covariates=series['test']['future'],\n",
    "                                        forecast_horizon=formatter.params['length_pred'], \n",
    "                                        stride=formatter.params['length_pred'] // 2,\n",
    "                                        retrain=False,\n",
    "                                        verbose=True,\n",
    "                                        last_points_only=False,\n",
    "                                        start=formatter.params['max_length_input'])\n",
    "id_errors_sample = rescale_and_backtest(series['test']['target'],\n",
    "                                        forecasts,  \n",
    "                                        [metrics.mse, metrics.mae],\n",
    "                                        scalers['target'],\n",
    "                                        reduction=None)\n",
    "id_errors_sample = np.vstack(id_errors_sample)\n",
    "np.median(id_errors_sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9af0babfa4fcc32151d0f9cd96f26ee8eefb724c47cfe9b27c84c1db30f6822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

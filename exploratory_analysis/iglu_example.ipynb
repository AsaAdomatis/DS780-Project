{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "from data_formatters.iglu import *\n",
    "from dataset import TSDataset\n",
    "from conf import Conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code walk-through\n",
    "\n",
    "The major parts of the code that need to be defined for each data set are:\n",
    "1. config file in `.yaml` format,\n",
    "2. data formatter script.\n",
    "\n",
    "For now, you can study the `electricity.yaml` example for a look of what a config file should feel like. You can skip the hyperparam defintions and the model parameters. The main focus would be on defining the dataset parameters. \n",
    "\n",
    "We do not intereact with `.yaml` in a direct way but instead though `Conf` class, which handles the following:\n",
    "1. defines some defaults if not specified in `.yaml`,\n",
    "2. sets save paths,\n",
    "3. allows for nice colored printing.\n",
    "\n",
    "Technically, we could doo all of this in the `.yaml` file directly. However, then every time we re-run the experiment, we would have to manually modify the `.yaml` file to reset save paths and redefine some variables, which would be inconvenient.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the config file, setting the experiment name, and the seed for random pre-processing parts (like splitting)\n",
    "cnf = Conf(conf_file_path='./conf/iglu.yaml', seed=15, exp_name=\"IGLU\", log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default configuration parameters: \n",
      "\u001b[34mLR\u001b[0m\u001b[31m: \u001b[0m\u001b[35m0.001\u001b[0m\n",
      "\u001b[34mEPOCHS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m20\u001b[0m\n",
      "\u001b[34mN_WORKERS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m0\u001b[0m\n",
      "\u001b[34mBATCH_SIZE\u001b[0m\u001b[31m: \u001b[0m\u001b[35m64\u001b[0m\n",
      "\u001b[34mQUANTILES\u001b[0m\u001b[31m: \u001b[0m\u001b[35m[0.1, 0.5, 0.9]\u001b[0m\n",
      "\u001b[34mDS_NAME\u001b[0m\u001b[31m: \u001b[0m\u001b[33miglu_urjeet\u001b[0m\n",
      "\u001b[34mALL_PARAMS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m{'ds_name': 'iglu_urjeet', 'data_csv_path': './raw_data/iglu_example_data_5_subject.csv', 'index_col': -1, 'total_time_steps': 192, 'num_encoder_steps': 168, 'max_samples': 5000, 'batch_size': 64, 'device': 'cuda', 'lr': 0.001, 'num_epochs': 20, 'n_workers': 0, 'model': 'transformer', 'loader': 'base', 'quantiles': [0.1, 0.5, 0.9], 'batch_first': True, 'early_stopping_patience': 5, 'hidden_layer_size': 160, 'stack_size': 1, 'dropout_rate': 0.1, 'max_gradient_norm': 0.01, 'num_heads': 4, 'd_model': 64, 'q': 16, 'v': 16, 'h': 4, 'N': 2, 'attention_size': 0, 'dropout': 0.1, 'pe': 'original', 'chunk_mode': 'None', 'd_input': 5, 'd_output': 3}\u001b[0m\n",
      "\u001b[34mEXP_LOG_PATH\u001b[0m\u001b[31m: \u001b[0m\u001b[33m./log\\transformer\\IGLU\\10/14/2022.08:20:21\u001b[0m\n",
      "\u001b[34mDEVICE\u001b[0m\u001b[31m: \u001b[0m\u001b[33mcpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# lets print out the config file\n",
    "print(f'\\nDefault configuration parameters: \\n{cnf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the data formatter. This is the part that should handle:\n",
    "1. loading the data and setting types,\n",
    "2. splitting the data into train / val / test sets,\n",
    "3. setting scalers and encoders for numerical / categorical variables resp.\n",
    "\n",
    "We are going to leave parts 2-3 for the future exploration. Now, let's focus on loading and settting the types for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped segments: 6\n",
      "Extracted segments: 26\n"
     ]
    }
   ],
   "source": [
    "# call the data fromatter directly\n",
    "data_formatter = IGLUFormatter(cnf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's work with the `TSDataset` class. This is the main part of the code as it aligns all of our previous steps. In the end, it is the `TSDataset` that is going to call the splitters, scalers, and encoders. **Importatnly** the model is only going to interact with the data through this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# we are going to pass our data formatter and the config file to the TSDataset class\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m TSDataset(cnf, data_formatter)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'split'"
     ]
    }
   ],
   "source": [
    "# we are going to pass our data formatter and the config file to the TSDataset class\n",
    "dataset = TSDataset(cnf, data_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #0: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #1: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #2: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #3: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #4: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #5: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #6: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #7: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #8: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #9: x.shape=(192, 1), y.shape=(192, 1)\n"
     ]
    }
   ],
   "source": [
    "# now let's see how we can sample minibatches from our dataset that we can then pass to the model to train on\n",
    "for i in range(10):\n",
    "    # 192 x ['power_usage', 'hour', 'day_of_week', 'hours_from_start', 'categorical_id']\n",
    "    x = dataset[i]['inputs']\n",
    "    # 24 x ['power_usage']\n",
    "    y = dataset[i]['outputs']\n",
    "    print(f'Example #{i}: x.shape={x.shape}, y.shape={y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae14cd44980bb66d0fa71274be0f56f60a32263b4377065a982a49ee9329494a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

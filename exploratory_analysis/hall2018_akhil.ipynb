{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_formatters.utils as utils\n",
    "from dataset import TSDataset\n",
    "from conf import Conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code walk-through\n",
    "\n",
    "The major parts of the code that need to be defined for each data set are:\n",
    "1. config file in `.yaml` format,\n",
    "2. data formatter script.\n",
    "\n",
    "For now, you can study the `electricity.yaml` example for a look of what a config file should feel like. You can skip the hyperparam defintions and the model parameters. The main focus would be on defining the dataset parameters. \n",
    "\n",
    "We do not intereact with `.yaml` in a direct way but instead though `Conf` class, which handles the following:\n",
    "1. defines some defaults if not specified in `.yaml`,\n",
    "2. sets save paths,\n",
    "3. allows for nice colored printing.\n",
    "\n",
    "Technically, we could doo all of this in the `.yaml` file directly. However, then every time we re-run the experiment, we would have to manually modify the `.yaml` file to reset save paths and redefine some variables, which would be inconvenient.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the config file, setting the experiment name, and the seed for random pre-processing parts (like splitting)\n",
    "cnf = Conf(conf_file_path='./conf/hall.yaml', seed=15, exp_name=\"test\", log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default configuration parameters: \n",
      "LR: 0.001\n",
      "EPOCHS: 20\n",
      "N_WORKERS: 0\n",
      "BATCH_SIZE: 64\n",
      "QUANTILES: [0.1, 0.5, 0.9]\n",
      "DS_NAME: hall2018_akhil\n",
      "ALL_PARAMS: {'ds_name': 'hall2018_akhil', 'data_csv_path': './raw_data/Hall2018_processed_akhil.csv', 'index_col': -1, 'total_time_steps': 192, 'num_encoder_steps': 168, 'max_samples': 0, 'batch_size': 64, 'device': 'cuda', 'lr': 0.001, 'num_epochs': 20, 'n_workers': 0, 'model': 'transformer', 'loader': 'base', 'quantiles': [0.1, 0.5, 0.9], 'batch_first': True, 'early_stopping_patience': 5, 'hidden_layer_size': 160, 'stack_size': 1, 'dropout_rate': 0.1, 'max_gradient_norm': 0.01, 'num_heads': 4, 'd_model': 64, 'q': 16, 'v': 16, 'h': 4, 'N': 2, 'attention_size': 0, 'dropout': 0.1, 'pe': 'original', 'chunk_mode': 'None', 'd_input': 5, 'd_output': 3}\n",
      "EXP_LOG_PATH: ./log\\transformer\\test\\09/23/2022.01:57:22\n",
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# lets print out the config file\n",
    "print(f'\\nDefault configuration parameters: \\n{cnf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the data formatter. This is the part that should handle:\n",
    "1. loading the data and setting types,\n",
    "2. splitting the data into train / val / test sets,\n",
    "3. setting scalers and encoders for numerical / categorical variables resp.\n",
    "\n",
    "We are going to leave parts 2-3 for the future exploration. Now, let's focus on loading and settting the types for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data formatter script for electricity is in electricity.py \n",
    "# we are going to use a helper function to get a pointer to the data formatter class for our dataset\n",
    "data_formatter = utils.make_data_formatter(cnf.ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's work with the `TSDataset` class. This is the main part of the code as it aligns all of our previous steps. In the end, it is the `TSDataset` that is going to call the splitters, scalers, and encoders. **Importatnly** the model is only going to interact with the data through this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting valid sampling locations.\n",
      "# available segments=94539\n",
      "Extracting all available segments.\n",
      "1000 of 94539 samples done...\n",
      "2000 of 94539 samples done...\n",
      "3000 of 94539 samples done...\n",
      "4000 of 94539 samples done...\n",
      "5000 of 94539 samples done...\n",
      "6000 of 94539 samples done...\n",
      "7000 of 94539 samples done...\n",
      "8000 of 94539 samples done...\n",
      "9000 of 94539 samples done...\n",
      "10000 of 94539 samples done...\n",
      "11000 of 94539 samples done...\n",
      "12000 of 94539 samples done...\n",
      "13000 of 94539 samples done...\n",
      "14000 of 94539 samples done...\n",
      "15000 of 94539 samples done...\n",
      "16000 of 94539 samples done...\n",
      "17000 of 94539 samples done...\n",
      "18000 of 94539 samples done...\n",
      "19000 of 94539 samples done...\n",
      "20000 of 94539 samples done...\n",
      "21000 of 94539 samples done...\n",
      "22000 of 94539 samples done...\n",
      "23000 of 94539 samples done...\n",
      "24000 of 94539 samples done...\n",
      "25000 of 94539 samples done...\n",
      "26000 of 94539 samples done...\n",
      "27000 of 94539 samples done...\n",
      "28000 of 94539 samples done...\n",
      "29000 of 94539 samples done...\n",
      "30000 of 94539 samples done...\n",
      "31000 of 94539 samples done...\n",
      "32000 of 94539 samples done...\n",
      "33000 of 94539 samples done...\n",
      "34000 of 94539 samples done...\n",
      "35000 of 94539 samples done...\n",
      "36000 of 94539 samples done...\n",
      "37000 of 94539 samples done...\n",
      "38000 of 94539 samples done...\n",
      "39000 of 94539 samples done...\n",
      "40000 of 94539 samples done...\n",
      "41000 of 94539 samples done...\n",
      "42000 of 94539 samples done...\n",
      "43000 of 94539 samples done...\n",
      "44000 of 94539 samples done...\n",
      "45000 of 94539 samples done...\n",
      "46000 of 94539 samples done...\n",
      "47000 of 94539 samples done...\n",
      "48000 of 94539 samples done...\n",
      "49000 of 94539 samples done...\n",
      "50000 of 94539 samples done...\n",
      "51000 of 94539 samples done...\n",
      "52000 of 94539 samples done...\n",
      "53000 of 94539 samples done...\n",
      "54000 of 94539 samples done...\n",
      "55000 of 94539 samples done...\n",
      "56000 of 94539 samples done...\n",
      "57000 of 94539 samples done...\n",
      "58000 of 94539 samples done...\n",
      "59000 of 94539 samples done...\n",
      "60000 of 94539 samples done...\n",
      "61000 of 94539 samples done...\n",
      "62000 of 94539 samples done...\n",
      "63000 of 94539 samples done...\n",
      "64000 of 94539 samples done...\n",
      "65000 of 94539 samples done...\n",
      "66000 of 94539 samples done...\n",
      "67000 of 94539 samples done...\n",
      "68000 of 94539 samples done...\n",
      "69000 of 94539 samples done...\n",
      "70000 of 94539 samples done...\n",
      "71000 of 94539 samples done...\n",
      "72000 of 94539 samples done...\n",
      "73000 of 94539 samples done...\n",
      "74000 of 94539 samples done...\n",
      "75000 of 94539 samples done...\n",
      "76000 of 94539 samples done...\n",
      "77000 of 94539 samples done...\n",
      "78000 of 94539 samples done...\n",
      "79000 of 94539 samples done...\n",
      "80000 of 94539 samples done...\n",
      "81000 of 94539 samples done...\n",
      "82000 of 94539 samples done...\n",
      "83000 of 94539 samples done...\n",
      "84000 of 94539 samples done...\n",
      "85000 of 94539 samples done...\n",
      "86000 of 94539 samples done...\n",
      "87000 of 94539 samples done...\n",
      "88000 of 94539 samples done...\n",
      "89000 of 94539 samples done...\n",
      "90000 of 94539 samples done...\n",
      "91000 of 94539 samples done...\n",
      "92000 of 94539 samples done...\n",
      "93000 of 94539 samples done...\n",
      "94000 of 94539 samples done...\n"
     ]
    }
   ],
   "source": [
    "# we are going to pass our data formatter and the config file to the TSDataset class\n",
    "dataset = TSDataset(cnf, data_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #0: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #1: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #2: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #3: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #4: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #5: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #6: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #7: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #8: x.shape=(192, 1), y.shape=(192, 1)\n",
      "Example #9: x.shape=(192, 1), y.shape=(192, 1)\n"
     ]
    }
   ],
   "source": [
    "# now let's see how we can sample minibatches from our dataset that we can then pass to the model to train on\n",
    "for i in range(10):\n",
    "    # 192 x ['power_usage', 'hour', 'day_of_week', 'hours_from_start', 'categorical_id']\n",
    "    x = dataset[i]['inputs']\n",
    "    # 24 x ['power_usage']\n",
    "    y = dataset[i]['outputs']\n",
    "    print(f'Example #{i}: x.shape={x.shape}, y.shape={y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
